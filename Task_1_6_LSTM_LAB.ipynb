{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using ProfileView\n",
    "using Knet, AutoGrad\n",
    "using Knet: sigm_dot, tanh_dot\n",
    "Profile.init(delay=0.01)\n",
    "# Check Task 8 notebook for profiling results Profile.init(delay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles    = [\"input.txt\"]  # If provided, use first file for training, second for dev, others for test.\n",
    "togenerate   = 500            # If non-zero generate given number of characters.\n",
    "epochs       = 10             # Number of epochs for training.\n",
    "hidden       = [128]          # Sizes of one or more LSTM layers.\n",
    "embed        = 168            # Size of the embedding vector.\n",
    "batchsize    = 128            # Number of sequences to train on in parallel\n",
    "seqlength    = 20             # Maximum number of steps to unroll the network for bptt. Initial epochs will use the epoch number as bptt length for faster convergence.\n",
    "seed         = -1             # Random number seed. -1 or 0 is no fixed seed\n",
    "lr           = 1e-1           # Initial learning rate\n",
    "gclip        = 3.0            # Value to clip the gradient norm at.\n",
    "dpout        = 0.0            # Dropout probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mChars read: Tuple{String,Int64}[(\"input.txt\", 105989)]\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "seed > 0 && srand(seed)\n",
    "\n",
    "# read text and report lengths\n",
    "text = map(readstring, datafiles)\n",
    "!isempty(text) && info(\"Chars read: $(map((f,c)->(basename(f),length(c)),datafiles,text))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-1: Create dictionary by completing createVocabulary function\n",
    "function createVocabulary takes text::Array{Any,1} that contains the names of datafiles you provided by opts[:datafiles] argument. It returns vocabulary::Dict{Char,Int}() for given text. In this lab, your text array is length of 1. For example the text is [\"content of input\"]. Note that for the sake of simplicity, we do NOT use validation or test dataset in this lab. You can try it by splitting your data into 3 different set after the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createVocabulary (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function createVocabulary(text)\n",
    "    vocab = Dict{Char,Int}()\n",
    "    # MY CODE STARTS HERE \n",
    "    \n",
    "    for (char_i,unique_character) in enumerate(unique(text[1]))\n",
    "        vocab[Char(unique_character)] = char_i\n",
    "    end\n",
    "    # MY CODE ENDS HERE\n",
    "    return vocab\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36m75 unique chars.\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "vocab = createVocabulary(text)\n",
    "info(\"$(length(vocab)) unique chars.\") # The output should be 75 unique chars for input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network function\n",
    "\n",
    "In a regular RNN - The core idea is to use past hidden weights, present input to calculate the next set of hidden state weights. i.e st = (Ux/t + W/st-1) . \n",
    "\n",
    "LSTM equations looks scary and there are a lot of them forget gate, ingate , output gate and change . But it's basically just another way to calculate the hidden states (except in this scenario the vanishing gradients is not an issue). \n",
    "\n",
    "In a LSTM network you have initialize four sets of weights Wf (Forget gate ft) Wi - input gage, Wo - Output, Wc - Change gate weights and the corresponding biases. So instead of creating new variables looks like this cell code is just creating one giant weights/gates and then split it into parts of equal width(columnwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    hsize   = size(hidden,2)\n",
    "    forget  = sigm_dot(gates[:,1:hsize])\n",
    "    ingate  = sigm_dot(gates[:,1+hsize:2hsize])\n",
    "    outgate = sigm_dot(gates[:,1+2hsize:3hsize])\n",
    "    change  = tanh_dot(gates[:,1+3hsize:end])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh_dot(cell)\n",
    "    return (hidden,cell)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-2: Create Initial weights\n",
    "initweights creates the weights and biases for the model. We are using LSTM network. We provide init function(for weights) and bias function(for bias)\n",
    "\n",
    "First we have to initialize weights from the placeholder here x=embed and embed being 168 looks like the input vector is 168 \n",
    "long vector. The first part of this model is the relationshp betwen concatted inputs and hidden weights .This belongs to the embedding layer. Also for multiple hidden layers, y1=cell(x) , y2= cell(y1).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(hidden, vocab, embed)\n",
    "    init(d...) = xavier(d...)\n",
    "    bias(d...) = zeros(d...)\n",
    "    model = Vector{Any}(2*length(hidden)+3)\n",
    "    X = embed\n",
    "    for k = 1:length(hidden)\n",
    "        # MY CODE STARTS HERE\n",
    "        #Concatted input and hidden layer weights\n",
    "        num_nodes_hidden = hidden[k]\n",
    "        model[2k-1]=init(X+num_nodes_hidden,4*num_nodes_hidden) #Because we have to initialize 4w's - wf,wi,wo,wc\n",
    "        model[2k]=bias(1,4*num_nodes_hidden)\n",
    "        X = num_nodes_hidden\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "    model[end-2] = init(vocab,embed)\n",
    "    model[end-1] = init(hidden[end],vocab)\n",
    "    model[end] = bias(1,vocab)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-3: Create Initial state\n",
    "At each time step, we take the hidden state from previous time step as input. To be able to do that,first we need to initialize hidden state. We also store updated hidden states in array created here. We initialize state as a zero matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initstate (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let blank = nothing; global initstate\n",
    "    function initstate(model, batch)\n",
    "        nlayers = div(length(model)-3,2)\n",
    "        state = Vector{Any}(2*nlayers)\n",
    "        for k = 1:nlayers\n",
    "            bias = model[2k]\n",
    "            hidden = div(length(bias),4)\n",
    "            if typeof(blank)!=typeof(bias) || size(blank)!=(batch,hidden)\n",
    "                blank = fill!(similar(bias, batch, hidden),0)\n",
    "            end\n",
    "            state[2k-1] = state[2k] = blank\n",
    "        end\n",
    "        return state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-4: Create Predict function\n",
    "predict is a function that takes w(model) created in initweights, s(state) created in initstate and input whose size is batchsize vocabulary You need to implement predict function for LSTM. You must use lstm function here. LSTM function is provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(model, state, newhidden; pdrop=0)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    newstate = similar(state)\n",
    "    for k = 1:nlayers\n",
    "        # MY CODE STARTS HERE\n",
    "        #newstate[2k-1] is the hidden layer (look at the initweights for more explanation)\n",
    "        newhidden = dropout(newhidden, pdrop)\n",
    "        (newstate[2k-1],newstate[2k])=lstm(model[2k-1],model[2k],state[2k-1],state[2k],newhidden)\n",
    "        newhidden = newstate[2k-1]\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "    return newhidden,newstate\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Sample function\n",
    "Generate function is a function we use to create some text that is similar to our training data. We provide sample function to you. You can predict the next character by using sample function once you calculate the probabilities given the input. index to char is the same dictionary as you created with createdictionary function but it works in the reverse direction. It gives you the character given the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate(model, tok2int, nchar)\n",
    "    int2tok = Vector{Char}(length(tok2int))\n",
    "    for (k,v) in tok2int; int2tok[v] = k; end\n",
    "    input = tok2int[' ']\n",
    "    state = initstate(model, 1)\n",
    "    for t in 1:nchar\n",
    "        embed = model[end-2][[input],:]\n",
    "        ypred,state = predict(model,state,embed)\n",
    "        ypred = ypred * model[end-1] .+ model[end]\n",
    "        input = sample(exp.(logp(ypred)))\n",
    "        print(int2tok[input])\n",
    "    end\n",
    "    println()\n",
    "end\n",
    "\n",
    "function sample(p)\n",
    "    p = convert(Array,p)\n",
    "    r = rand()\n",
    "    for c = 1:length(p)\n",
    "        r -= p[c]\n",
    "        r < 0 && return c\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, Let's generate some random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## RANDOM MODEL OUTPUT ############\n",
      "iz12'I4?2tNUb23EYocU!r0pY*mdo4 K1nhP6p*NsF1KY59xdJd.iz:eecTfQ}W*Jr1bToV?Mu}*sFL5Kb8lEL{nkPsW3YjNIh?HaN3rS-N8guP6{Fo6V\\gf2G'wyze5Mv1lRHG-;g&7.gl,NsnmMp'd SBN*m3Say2pkqyYv-!J?lBy6.wNstW6tO\\I2ee SUrlB:OM&UMaIqT8MR4{\\BPAq'CxQ8{MONFrfgIyxD,fCzlFADpOpUo\\m{i?CWbVI\n",
      "wde4lhr&PHUcgKzTCf9YBLJSQUmC.!eIJ xuHO 8Pj!5jNaEWa5wT77iF&EnsY1,PiT\n",
      "y3aJrz0;lsfJsU{Mn42P\\:k7C-4lQNKWBkVzCQt-ptTVDpov\n",
      "jt3ji2WyEx\n",
      "*VBJH\\7f{,My!v'g8sew\n",
      ":6sIci0.HLghDdT2k8Ce8Kd7\n",
      "wjpECinjP}l-1h!sS-vG2:b{w*gVrbHBl:m\\y31PP5M ;PeLgjkAAH4Jf7ngk;H!B'{!\n"
     ]
    }
   ],
   "source": [
    "model = initweights(hidden, length(vocab), embed)\n",
    "state = initstate(model,1)\n",
    "\n",
    "println(\"########## RANDOM MODEL OUTPUT ############\")\n",
    "generate(model, vocab, togenerate) ## change togenerate if you want longer sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide minibatch function for you. You do not have to do it for this lab. But we suggest you to understand the idea since you need to do it in your own project and future labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function minibatch(chars, tok2int, batch_size)\n",
    "    chars = collect(chars)\n",
    "    nbatch = div(length(chars), batch_size)\n",
    "    data = [zeros(Int,batch_size) for i=1:nbatch ]\n",
    "    for n = 1:nbatch\n",
    "        for b = 1:batch_size\n",
    "            char = chars[(b-1)*nbatch + n]\n",
    "            data[n][b] = tok2int[char]\n",
    "        end\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-5: Create loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgloss (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, state, sequence, range=1:length(sequence)-1; newstate=nothing, pdrop=0)\n",
    "    preds = []\n",
    "    for t in range\n",
    "        input = model[end-2][sequence[t],:]\n",
    "        pred,state = predict(model,state,input; pdrop=pdrop)\n",
    "        push!(preds,pred)\n",
    "    end\n",
    "    if newstate != nothing\n",
    "        copy!(newstate, map(AutoGrad.getval,state))\n",
    "    end\n",
    "    pred0 = vcat(preds...)\n",
    "    pred1 = dropout(pred0,pdrop)\n",
    "    pred2 = pred1 * model[end-1]\n",
    "    pred3 = pred2 .+ model[end]\n",
    "    logp1 = logp(pred3,2)\n",
    "    nrows,ncols = size(pred3)\n",
    "    golds = vcat(sequence[range[1]+1:range[end]+1]...)\n",
    "    index = similar(golds)\n",
    "    @inbounds for i=1:length(golds)\n",
    "        index[i] = i + (golds[i]-1)*nrows\n",
    "    end\n",
    "    logp2 = logp1[index]\n",
    "    logp3 = sum(logp2)\n",
    "    return -logp3 / length(golds)\n",
    "end\n",
    "\n",
    "# Knet magic\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function avgloss(model, sequence, S)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    total = count = 0\n",
    "    for i in 1:S:T-1\n",
    "        j = min(i+S-1,T-1)\n",
    "        n = j-i+1\n",
    "        total += n * loss(model, state, sequence, i:j; newstate=state)\n",
    "        count += n\n",
    "    end\n",
    "    return total / count\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-6: Create Train function¶\n",
    "Implement bptt(Backpropagation through time) function for training. You need to fill up only 3 lines(or even small numbers). You need use lossgradient function and update! function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(model, sequence, optim, S; pdrop=0)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    for i in 1:S:T-1\n",
    "        # MY CODE STARTS HERE\n",
    "        end_seq = 1+S-1\n",
    "        if end_seq > T-1\n",
    "            end_seq = T-1\n",
    "        end\n",
    "        gradient_loss = lossgradient(model,state,sequence,1:end_seq,newstate=state,pdrop=pdrop)\n",
    "        update!(model,gradient_loss,optim)\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are ready. First let's see the initial loss¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 0, :loss, 4.317592514297592)\n"
     ]
    }
   ],
   "source": [
    "data =  map(t->minibatch(t, vocab, batchsize), text)\n",
    "# Print the loss of randomly initialized model.\n",
    "losses = map(d->avgloss(model,d,100), data)\n",
    "println((:epoch,0,:loss,losses...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the training part of RNN(with Adam)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18.338754 seconds (3.76 M allocations: 13.204 GiB, 8.90% gc time)\n",
      "(:epoch, 1, :loss, 6.780125855629913)\n",
      " 30.590885 seconds (1.82 M allocations: 10.573 GiB, 4.72% gc time)\n",
      "(:epoch, 2, :loss, 16.26163632385984)\n",
      " 29.128722 seconds (1.63 M allocations: 9.716 GiB, 5.74% gc time)\n",
      "(:epoch, 3, :loss, 17.49196940434865)\n",
      " 27.633746 seconds (1.56 M allocations: 9.288 GiB, 6.63% gc time)\n",
      "(:epoch, 4, :loss, 15.865533924255187)\n",
      " 30.773335 seconds (1.56 M allocations: 9.053 GiB, 17.83% gc time)\n",
      "(:epoch, 5, :loss, 14.818047866292815)\n",
      " 30.117670 seconds (1.56 M allocations: 8.861 GiB, 15.89% gc time)\n",
      "(:epoch, 6, :loss, 14.119376676442887)\n",
      " 32.266402 seconds (1.58 M allocations: 8.792 GiB, 20.91% gc time)\n",
      "(:epoch, 7, :loss, 12.166853469777411)\n",
      " 33.595647 seconds (1.58 M allocations: 8.689 GiB, 23.91% gc time)\n",
      "(:epoch, 8, :loss, 10.635709914923337)\n",
      " 32.038054 seconds (1.58 M allocations: 8.576 GiB, 27.06% gc time)\n",
      "(:epoch, 9, :loss, 9.597752328104672)\n",
      " 32.814754 seconds (1.58 M allocations: 8.540 GiB, 25.12% gc time)\n",
      "(:epoch, 10, :loss, 9.188941306946734)\n"
     ]
    }
   ],
   "source": [
    "optim = map(x->Adam(lr=lr, gclip=gclip), model)\n",
    "# MAIN LOOP\n",
    "function trainingloop()\n",
    "    for epoch=1:epochs\n",
    "        @time train(model, data[1], optim, min(epoch,seqlength); pdrop=dpout)\n",
    "        # Calculate and print the losses after each epoch\n",
    "        losses = map(d->avgloss(model,d,100),data)\n",
    "        println((:epoch,epoch,:loss,losses...))\n",
    "    end\n",
    "end\n",
    "trainingloop()\n",
    "# Profile.clear_malloc_data()\n",
    "# @profile trainingloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you have checked the loss decreasing, let's create some text with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## FINAL  MODEL OUTPUT ############\n",
      "hate walon she fiven thelspor fore gringr the was rthere them de'ste,\\\n",
      "Ast, Atences, likile.\\\n",
      "Thorthe powell o' ase; s Agrippod frain'd,\\\n",
      "plorwy;\\\n",
      "Len:\\\n",
      "We,\\\n",
      "Ands! fre ghe ther th hat thion s bodyor ye wer sa,, dshe hat the hat those yeret dam, whole grinor th o'erwinkifwerspeparthe power a br bontallft n to betthe am conn'd brv,'twahem de'st, wiftnner.\\\n",
      "Mashe the theree ve vensing?sipowrv, be ds, wllld t those grinrsponst too?\\\n",
      "\\\n",
      "First t?\\\n",
      "\\\n",
      "F count wan nough ee'enttrien the pen Ag inil !\\\n",
      "The \n"
     ]
    }
   ],
   "source": [
    "println(\"########## FINAL  MODEL OUTPUT ############\")\n",
    "state = initstate(model,1)\n",
    "generate(model, vocab, togenerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open(\"cpu_profile.bin\", \"w\") do f serialize(f, Profile.retrieve()) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
