{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "let _build = false\n",
    "    for p in (\"AutoGrad\",\"ArgParse\",\"Compat\",\"JLD\",\"Knet\")\n",
    "        if Pkg.installed(p) == nothing\n",
    "            Pkg.add(p)\n",
    "            _build = true\n",
    "        end\n",
    "    end\n",
    "    if _build\n",
    "        Pkg.build(\"Knet\")\n",
    "    end\n",
    "end\n",
    "\"\"\"\n",
    "\n",
    "This example implements an LSTM network for training and testing\n",
    "character-level language models inspired by [\"The Unreasonable\n",
    "Effectiveness of Recurrent Neural\n",
    "Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness)\n",
    "from Andrej Karpathy's blog.  The model can be trained with different\n",
    "genres of text, and can be used to generate original text in the same\n",
    "style.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "* `julia charlm.jl`: trains a model using its own code.\n",
    "\n",
    "* `julia charlm.jl --data foo.txt`: uses foo.txt to train instead.\n",
    "\n",
    "* `julia charlm.jl --data foo.txt bar.txt`: uses foo.txt for training\n",
    "  and bar.txt for validation.  Any number of files can be specified,\n",
    "  the first two will be used for training and validation, the rest for\n",
    "  testing.\n",
    "\n",
    "* `julia charlm.jl --best foo.jld --save bar.jld`: saves the best\n",
    "  model (according to validation set) to foo.jld, last model to\n",
    "  bar.jld.\n",
    "\n",
    "* `julia charlm.jl --load foo.jld --generate 1000 --sresult generated.txt`:\n",
    "  generates 1000 characters from the model in foo.jld and saves it to\n",
    "  generated.txt.\n",
    "\n",
    "\n",
    "* `julia charlm.jl --help`: describes all available options.\n",
    "\n",
    "\"\"\"\n",
    "module CharLM\n",
    "using Knet,AutoGrad,ArgParse,Compat,JLD\n",
    "using Knet: sigm_dot, tanh_dot\n",
    "\n",
    "# LSTM implementation with a single matrix multiplication with\n",
    "# instances in rows rather than columns.  Julia is column major, so\n",
    "# horizontal concatenation and column based slicing are contiguous and\n",
    "# more efficient compared to vertical concatenation and row\n",
    "# slicing. In this implementation I wanted to perform a single matrix\n",
    "# multiplication for all four gates rather than four (or eight)\n",
    "# separate matrix multiplications for performance. Thus I concatenate\n",
    "# the input and the hidden, then slice out the four gates.  Both\n",
    "# operations are more efficient if instances are in rows rather than\n",
    "# columns.\n",
    "\n",
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    hsize   = size(hidden,2)\n",
    "    forget  = sigm_dot(gates[:,1:hsize])\n",
    "    ingate  = sigm_dot(gates[:,1+hsize:2hsize])\n",
    "    outgate = sigm_dot(gates[:,1+2hsize:3hsize])\n",
    "    change  = tanh_dot(gates[:,1+3hsize:end])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh_dot(cell)\n",
    "    return (hidden,cell)\n",
    "end\n",
    "\n",
    "# generate model parameters for k=1:length(hidden) lstm layers\n",
    "# instances are in rows, vectors are row vectors\n",
    "# model[2k-1]: weight matrix for the k'th lstm layer\n",
    "# model[2k]: bias vector for the k'th lstm layer\n",
    "# model[end-2]: embedding matrix\n",
    "# model[end-1,end]: weight and bias for final prediction\n",
    "function initmodel(atype, hidden, vocab, embed)\n",
    "    init(d...)=atype(xavier(d...))\n",
    "    bias(d...)=atype(zeros(d...))\n",
    "    model = Array{Any}(2*length(hidden)+3)\n",
    "    X = embed\n",
    "    for k = 1:length(hidden)\n",
    "        H = hidden[k]\n",
    "        model[2k-1] = init(X+H, 4H)\n",
    "        model[2k] = bias(1, 4H)\n",
    "        model[2k][1:H] = 1 # forget gate bias = 1\n",
    "        X = H\n",
    "    end\n",
    "    model[end-2] = init(vocab,embed)\n",
    "    model[end-1] = init(hidden[end],vocab)\n",
    "    model[end] = bias(1,vocab)\n",
    "    return model\n",
    "end\n",
    "\n",
    "# state[2k-1]: hidden for the k'th lstm layer\n",
    "# state[2k]: cell for the k'th lstm layer\n",
    "let blank = nothing; global initstate\n",
    "function initstate(model, batch)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    state = Array{Any}(2*nlayers)\n",
    "    for k = 1:nlayers\n",
    "        bias = model[2k]\n",
    "        hidden = div(length(bias),4)\n",
    "        if typeof(blank)!=typeof(bias) || size(blank)!=(batch,hidden)\n",
    "            blank = fill!(similar(bias, batch, hidden),0)\n",
    "        end\n",
    "        state[2k-1] = state[2k] = blank\n",
    "    end\n",
    "    return state\n",
    "end\n",
    "end\n",
    "\n",
    "# initoptim creates optimization parameters for each numeric weight\n",
    "# array in the model.  This should work for a model consisting of any\n",
    "# combination of tuple/array/dict.\n",
    "initoptim{T<:Number}(::KnetArray{T},otype)=eval(parse(otype))\n",
    "initoptim{T<:Number}(::Array{T},otype)=eval(parse(otype))\n",
    "# TODO: This breaks Julia4 parser:\n",
    "# initoptim(a::Associative,otype)=Dict(k=>initoptim(v,otype) for (k,v) in a)\n",
    "initoptim(a,otype)=map(x->initoptim(x,otype), a)\n",
    "\n",
    "# input: Dense token-minibatch input\n",
    "# returns (B,H) hidden output and newstate\n",
    "function predict(model, state, input; pdrop=0)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    newstate = similar(state)\n",
    "    for k = 1:nlayers\n",
    "        input = dropout(input, pdrop)\n",
    "        (newstate[2k-1],newstate[2k]) = lstm(model[2k-1],model[2k],state[2k-1],state[2k],input)\n",
    "        input = newstate[2k-1]\n",
    "    end\n",
    "    return input,newstate\n",
    "end\n",
    "\n",
    "function generate(model, tok2int, nchar)\n",
    "    int2tok = Array{Char}(length(tok2int))\n",
    "    for (k,v) in tok2int; int2tok[v] = k; end\n",
    "    input = tok2int[' ']\n",
    "    state = initstate(model, 1)\n",
    "    # Open file for saving\n",
    "    if o[:sresult] != nothing\n",
    "        f = open(o[:sresult],\"w\")\n",
    "    end\n",
    "    for t in 1:nchar\n",
    "        embed = model[end-2][[input],:]\n",
    "        ypred,state = predict(model,state,embed)\n",
    "        ypred = ypred * model[end-1] .+ model[end]\n",
    "        input = sample(exp.(logp(ypred)))\n",
    "        print(int2tok[input])\n",
    "        # Save character to file\n",
    "        if o[:sresult] != nothing\n",
    "            write(f, int2tok[input])\n",
    "        end\n",
    "    end\n",
    "    # Close file if opened\n",
    "    if o[:sresult] != nothing\n",
    "        close(f)\n",
    "    end\n",
    "    println()\n",
    "end\n",
    "\n",
    "# sequence[t]: Vector{Int} token-minibatch input at time t\n",
    "function loss(model, state, sequence, range=1:length(sequence)-1; newstate=nothing, pdrop=0)\n",
    "    preds = []\n",
    "    for t in range\n",
    "        input = model[end-2][sequence[t],:]\n",
    "        pred,state = predict(model,state,input; pdrop=pdrop)\n",
    "        push!(preds,pred)\n",
    "    end\n",
    "    if newstate != nothing\n",
    "        copy!(newstate, map(AutoGrad.getval,state))\n",
    "    end\n",
    "    pred0 = vcat(preds...)\n",
    "    pred1 = dropout(pred0,pdrop)\n",
    "    pred2 = pred1 * model[end-1]\n",
    "    pred3 = pred2 .+ model[end]\n",
    "    logp1 = logp(pred3,2)\n",
    "    nrows,ncols = size(pred3)\n",
    "    golds = vcat(sequence[range[1]+1:range[end]+1]...)\n",
    "    index = similar(golds)\n",
    "    @inbounds for i=1:length(golds)\n",
    "        index[i] = i + (golds[i]-1)*nrows\n",
    "    end\n",
    "    # pred3 = Array(pred3) #TODO: FIX BUGGY REDUCTION CODE FOR KNETARRAY IF PRED3 TOO BIG\n",
    "    logp2 = logp1[index]\n",
    "    logp3 = sum(logp2)\n",
    "    return -logp3 / length(golds)\n",
    "end\n",
    "\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function avgloss(model, sequence, S)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    total = count = 0\n",
    "    for i in 1:S:T-1\n",
    "        j = min(i+S-1,T-1)\n",
    "        n = j-i+1\n",
    "        total += n * loss(model, state, sequence, i:j; newstate=state)\n",
    "        count += n\n",
    "    end\n",
    "    return total / count\n",
    "end\n",
    "\n",
    "function bptt(model, sequence, optim, S; pdrop=0)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    for i in 1:S:T-1\n",
    "        j = min(i+S-1,T-1)\n",
    "        grads = lossgradient(model, state, sequence, i:j; newstate=state, pdrop=pdrop)\n",
    "        update!(model, grads, optim)\n",
    "    end\n",
    "end\n",
    "\n",
    "# It turns out convergence is much faster if we keep bptt length small\n",
    "# initially. So we take user's o[:seqlength] as the upper limit and\n",
    "# bound the bptt length by the epoch number.\n",
    "function train!(model, data, tok2int, o)\n",
    "    global optim = initoptim(model,o[:optimization])\n",
    "    if o[:fast]\n",
    "        for epoch=1:o[:epochs]\n",
    "            bptt(model, data[1], optim, min(epoch,o[:seqlength]); pdrop=o[:dropout])\n",
    "        end\n",
    "        return\n",
    "    end\n",
    "    report(ep)=(l=map(d->avgloss(model,d,100), data);println((:epoch,ep,:loss,l...));l)\n",
    "    @time losses = report(0)\n",
    "    devset = ifelse(length(data) > 1, 2, 1)\n",
    "    devlast = devbest = losses[devset]\n",
    "    for epoch=1:o[:epochs]\n",
    "        @time bptt(model, data[1], optim, min(epoch,o[:seqlength]); pdrop=o[:dropout])\n",
    "        @time losses = report(epoch)\n",
    "        if o[:gcheck] > 0\n",
    "            gradcheck(loss, model, data[1], 1:min(o[:seqlength],length(data[1])-1); gcheck=o[:gcheck], verbose=true)\n",
    "        end\n",
    "        devloss = losses[devset]\n",
    "        if devloss < devbest\n",
    "            devbest = devloss\n",
    "            if o[:bestfile] != nothing\n",
    "                info(\"Saving best model to $(o[:bestfile])\")\n",
    "                save(o[:bestfile], \"model\", model, \"vocab\", tok2int)\n",
    "            end\n",
    "        end\n",
    "        devlast = devloss\n",
    "    end\n",
    "    if o[:savefile] != nothing\n",
    "        info(\"Saving final model to $(o[:savefile])\")\n",
    "        save(o[:savefile], \"model\", model, \"vocab\", tok2int)\n",
    "    end\n",
    "end\n",
    "\n",
    "function minibatch(chars, tok2int, batch_size)\n",
    "    chars = collect(chars)\n",
    "    nbatch = div(length(chars), batch_size)\n",
    "    data = [ zeros(Int,batch_size) for i=1:nbatch ]\n",
    "    for n = 1:nbatch\n",
    "        for b = 1:batch_size\n",
    "            char = chars[(b-1)*nbatch + n]\n",
    "            data[n][b] = tok2int[char]\n",
    "        end\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "function sample(p)\n",
    "    p = convert(Array,p)\n",
    "    r = rand()\n",
    "    for c = 1:length(p)\n",
    "        r -= p[c]\n",
    "        r < 0 && return c\n",
    "    end\n",
    "end\n",
    "\n",
    "function shakespeare()\n",
    "    file = Knet.dir(\"data\",\"100.txt\")\n",
    "    if !isfile(file)\n",
    "        info(\"Downloading 'The Complete Works of William Shakespeare'\")\n",
    "        url = \"http://www.gutenberg.org/files/100/100.txt\"\n",
    "        download(url,file)\n",
    "    end\n",
    "    return file\n",
    "end\n",
    "\n",
    "function main(args=ARGS)\n",
    "    global model, text, data, tok2int, o\n",
    "    s = ArgParseSettings()\n",
    "    s.description=\"charlm.jl: Character level language model based on http://karpathy.github.io/2015/05/21/rnn-effectiveness. (c) Emre Yolcu, Deniz Yuret, 2017.\"\n",
    "    s.exc_handler=ArgParse.debug_handler\n",
    "    @add_arg_table s begin\n",
    "        (\"--datafiles\"; nargs='+'; help=\"If provided, use first file for training, second for dev, others for test.\")\n",
    "        (\"--loadfile\"; help=\"Initialize model from file\")\n",
    "        (\"--savefile\"; help=\"Save final model to file\")\n",
    "        (\"--bestfile\"; help=\"Save best model to file\")\n",
    "        (\"--generate\"; arg_type=Int; default=0; help=\"If non-zero generate given number of characters.\")\n",
    "        (\"--epochs\"; arg_type=Int; default=1; help=\"Number of epochs for training.\")\n",
    "        (\"--hidden\"; nargs='+'; arg_type=Int; default=[334]; help=\"Sizes of one or more LSTM layers.\")\n",
    "        (\"--embed\"; arg_type=Int; default=168; help=\"Size of the embedding vector.\")\n",
    "        (\"--batchsize\"; arg_type=Int; default=256; help=\"Number of sequences to train on in parallel.\")\n",
    "        (\"--seqlength\"; arg_type=Int; default=100; help=\"Maximum number of steps to unroll the network for bptt. Initial epochs will use the epoch number as bptt length for faster convergence.\")\n",
    "        (\"--optimization\"; default=\"Adam()\"; help=\"Optimization algorithm and parameters.\")\n",
    "        (\"--gcheck\"; arg_type=Int; default=0; help=\"Check N random gradients.\")\n",
    "        (\"--seed\"; arg_type=Int; default=-1; help=\"Random number seed.\")\n",
    "        (\"--atype\"; default=(gpu()>=0 ? \"KnetArray{Float32}\" : \"Array{Float32}\"); help=\"array type: Array for cpu, KnetArray for gpu\")\n",
    "        (\"--fast\"; action=:store_true; help=\"skip loss printing for faster run\")\n",
    "        (\"--dropout\"; arg_type=Float64; default=0.0; help=\"Dropout probability.\")\n",
    "        (\"--sresult\"; help = \"Save generated text to file\" )\n",
    "    end\n",
    "    isa(args, AbstractString) && (args=split(args))\n",
    "    o = parse_args(args, s; as_symbols=true)\n",
    "    if !o[:fast]\n",
    "        println(s.description)\n",
    "        println(\"opts=\",[(k,v) for (k,v) in o]...)\n",
    "    end\n",
    "    o[:seed] > 0 && srand(o[:seed])\n",
    "    o[:atype] = eval(parse(o[:atype]))\n",
    "\n",
    "    # we initialize a model from loadfile, train using datafiles (both optional).\n",
    "    # if the user specifies neither, train a model using the charlm.jl source code.\n",
    "    isempty(o[:datafiles]) && o[:loadfile]==nothing && push!(o[:datafiles],@__FILE__) # shakespeare()\n",
    "\n",
    "    # read text and report lengths\n",
    "    text = map((@compat readstring), o[:datafiles])\n",
    "    !isempty(text) && !o[:fast] && info(\"Chars read: $(map((f,c)->(basename(f),length(c)),o[:datafiles],text))\")\n",
    "\n",
    "    # tok2int (char_to_index) comes from the initial model if there is one, otherwise from the datafiles.\n",
    "    # if there is an initial model make sure the data has no new vocab\n",
    "    if o[:loadfile]==nothing\n",
    "        tok2int = Dict{Char,Int}()\n",
    "        for t in text, c in t; get!(tok2int, c, 1+length(tok2int)); end\n",
    "        model = initmodel(o[:atype], o[:hidden], length(tok2int), o[:embed])\n",
    "    else\n",
    "        info(\"Loading model from $(o[:loadfile])\")\n",
    "        tok2int = load(o[:loadfile], \"vocab\")\n",
    "        for t in text, c in t; haskey(tok2int, c) || error(\"Unknown char $c\"); end\n",
    "        model = map(p->convert(o[:atype],p), load(o[:loadfile], \"model\"))\n",
    "    end\n",
    "    !o[:fast] && info(\"$(length(tok2int)) unique chars.\")\n",
    "    if !isempty(text)\n",
    "        data = map(t->minibatch(t, tok2int, o[:batchsize]), text)\n",
    "        train!(model, data, tok2int, o)\n",
    "    end\n",
    "    if o[:generate] > 0\n",
    "        generate(model, tok2int, o[:generate])\n",
    "    end\n",
    "    return model\n",
    "end\n",
    "\n",
    "\n",
    "# This allows both non-interactive (shell command) and interactive calls like:\n",
    "# $ julia charlm.jl --epochs 10\n",
    "# julia> CharLM.main(\"--epochs 10\")\n",
    "if VERSION >= v\"0.5.0-dev+7720\"\n",
    "    PROGRAM_FILE==\"charlm.jl\" && main(ARGS)\n",
    "else\n",
    "    !isinteractive() && !isdefined(Core.Main,:load_only) && main(ARGS)\n",
    "end\n",
    "\n",
    "end  # module\n",
    "\n",
    "# Note: 10.txt used in the sample runs below was generated using\n",
    "#   head -10000 100.txt > 10.txt\n",
    "# where 100.txt is the file downloaded by shakespeare().\n",
    "\n",
    "\n",
    "\n",
    "### SAMPLE RUN 74a2e6c+ Mon Sep 19 14:03:10 EEST 2016\n",
    "### Implemented multi-layer.  Removed the keepstate option fixing it to true.\n",
    "### Note that winit default changed so I specify it below for comparison.\n",
    "### The slight difference is due to keepstate.\n",
    "\n",
    "# julia> CharLM.main(\"--data 10.txt --winit 0.3 --fast\")\n",
    "# charlm.jl (c) Emre Yolcu, Deniz Yuret, 2016. Character level language model based on http://karpathy.github.io/2015/05/21/rnn-effectiveness.\n",
    "# opts=(:lr,4.0)(:atype,\"KnetArray{Float32}\")(:winit,0.3)(:savefile,nothing)(:loadfile,nothing)(:generate,0)(:bestfile,nothing)(:gclip,3.0)(:hidden,[256])(:epochs,3)(:decay,0.9)(:gcheck,0)(:seqlength,100)(:seed,42)(:embed,256)(:batchsize,128)(:datafiles,Any[\"10.txt\"])(:fast,true)\n",
    "# INFO: Chars read: [(\"10.txt\",425808)]\n",
    "# INFO: 87 unique chars.\n",
    "#   1.406687 seconds (1.74 M allocations: 196.799 MB, 2.49% gc time)\n",
    "# (:epoch,0,:loss,6.1075509900258)\n",
    "#   4.002638 seconds (6.12 M allocations: 374.188 MB, 2.41% gc time)\n",
    "#   3.990772 seconds (6.11 M allocations: 374.129 MB, 2.44% gc time)\n",
    "#   4.006249 seconds (6.12 M allocations: 374.240 MB, 2.53% gc time)\n",
    "#   1.405878 seconds (1.75 M allocations: 197.059 MB, 2.56% gc time)\n",
    "# (:epoch,3,:loss,1.8713183968407767)\n",
    "\n",
    "\n",
    "\n",
    "### SAMPLE RUN 4ce58d1+ Fri Sep 16 12:24:00 EEST 2016\n",
    "### Transposed everything so getindex does not need to copy\n",
    "\n",
    "# charlm.jl (c) Emre Yolcu, Deniz Yuret, 2016. Character level language model based on http://karpathy.github.io/2015/05/21/rnn-effectiveness.\n",
    "# opts=(:keepstate,false)(:lr,4.0)(:atype,\"KnetArray{Float32}\")(:winit,0.3)(:savefile,nothing)(:loadfile,nothing)(:generate,0)(:bestfile,nothing)(:gclip,3.0)(:embedding,256)(:hidden,256)(:epochs,3)(:decay,0.9)(:gcheck,0)(:seqlength,100)(:seed,42)(:batchsize,128)(:datafiles,Any[\"data/10.txt\"])(:fast,true)\n",
    "# INFO: Chars read: [(\"10.txt\",425808)]\n",
    "# INFO: 87 unique chars.\n",
    "#   1.388652 seconds (1.73 M allocations: 196.686 MB, 2.04% gc time)\n",
    "# (:epoch,0,:loss,6.1075509900258)\n",
    "#   3.940298 seconds (6.06 M allocations: 373.166 MB, 2.06% gc time)\n",
    "#   3.935995 seconds (6.06 M allocations: 373.244 MB, 2.07% gc time)\n",
    "#   3.934983 seconds (6.06 M allocations: 373.245 MB, 2.08% gc time)\n",
    "#   1.390374 seconds (1.73 M allocations: 196.820 MB, 2.11% gc time)\n",
    "# (:epoch,3,:loss,1.860654126576015)\n",
    "\n",
    "\n",
    "\n",
    "### SAMPLE RUN 31136d5+ Wed Sep 14 17:51:44 EEST 2016: using vcat(x,h) and vcat(w...)\n",
    "### optimized learning parameters: winit=0.3, lr=4.0, gclip=3.0\n",
    "\n",
    "# charlm.jl (c) Emre Yolcu, Deniz Yuret, 2016. Character level language model based on http://karpathy.github.io/2015/05/21/rnn-effectiveness.\n",
    "# opts=(:keepstate,false)(:lr,4.0)(:atype,\"KnetArray{Float32}\")(:winit,0.3)(:savefile,nothing)(:loadfile,nothing)(:generate,0)(:bestfile,nothing)(:gclip,3.0)(:embedding,256)(:hidden,256)(:epochs,3)(:decay,0.9)(:gcheck,0)(:seqlength,100)(:seed,42)(:batchsize,128)(:datafiles,Any[\"10.txt\"])(:fast,true)\n",
    "# INFO: Chars read: [(\"10.txt\",425808)]\n",
    "# INFO: 87 unique chars.\n",
    "#   1.596959 seconds (1.79 M allocations: 197.432 MB, 2.56% gc time)\n",
    "# (:epoch,0,:loss,5.541976199042528)\n",
    "#   4.421566 seconds (6.23 M allocations: 375.843 MB, 2.54% gc time)\n",
    "#   4.418540 seconds (6.25 M allocations: 376.058 MB, 2.51% gc time)\n",
    "#   4.402317 seconds (6.26 M allocations: 376.297 MB, 2.66% gc time)\n",
    "#   1.594677 seconds (1.81 M allocations: 197.737 MB, 2.64% gc time)\n",
    "# (:epoch,3,:loss,1.8484550957572192)\n",
    "\n",
    "\n",
    "### SAMPLE RUN 80503e7+ Wed Sep 14 17:35:36 EEST 2016: using vcat(x,h)\n",
    "#\n",
    "# charlm.jl (c) Emre Yolcu, Deniz Yuret, 2016. Character level language model based on http://karpathy.github.io/2015/05/21/rnn-effectiveness.\n",
    "# opts=(:keepstate,false)(:lr,1.0)(:atype,\"KnetArray{Float32}\")(:savefile,nothing)(:loadfile,nothing)(:generate,0)(:bestfile,nothing)(:embedding,256)(:gclip,5.0)(:hidden,256)(:epochs,3)(:decay,0.9)(:gcheck,0)(:seqlength,100)(:seed,42)(:batchsize,128)(:datafiles,Any[\"10.txt\"])(:fast,true)\n",
    "# INFO: Chars read: [(\"10.txt\",425808)]\n",
    "# INFO: 87 unique chars.\n",
    "#   1.930180 seconds (1.95 M allocations: 213.741 MB, 1.82% gc time)\n",
    "# (:epoch,0,:loss,4.462641664662756)\n",
    "#   4.968101 seconds (7.47 M allocations: 454.259 MB, 2.24% gc time)\n",
    "#   4.963733 seconds (7.47 M allocations: 454.363 MB, 2.26% gc time)\n",
    "#   4.967413 seconds (7.45 M allocations: 454.024 MB, 2.14% gc time)\n",
    "#   1.945658 seconds (1.98 M allocations: 214.183 MB, 2.02% gc time)\n",
    "# (:epoch,3,:loss,3.2389672966290237)\n",
    "\n",
    "\n",
    "### SAMPLE RUN 65f57ff+ Wed Sep 14 10:02:30 EEST 2016: separate x, h, w, b\n",
    "#\n",
    "# charlm.jl (c) Emre Yolcu, Deniz Yuret, 2016. Character level language model based on http://karpathy.github.io/2015/05/21/rnn-effectiveness.\n",
    "# opts=(:keepstate,false)(:lr,1.0)(:atype,\"KnetArray{Float32}\")(:savefile,nothing)(:loadfile,nothing)(:generate,0)(:bestfile,nothing)(:embedding,256)(:gclip,5.0)(:hidden,256)(:epochs,3)(:decay,0.9)(:gcheck,0)(:seqlength,100)(:seed,42)(:batchsize,128)(:datafiles,Any[\"10.txt\"])(:fast,true)\n",
    "# INFO: Chars read: [(\"10.txt\",425808)]\n",
    "# INFO: 87 unique chars.\n",
    "#   2.156358 seconds (2.31 M allocations: 237.913 MB, 2.30% gc time)\n",
    "# (:epoch,0,:loss,4.465127425659868)\n",
    "#   6.287736 seconds (9.54 M allocations: 574.703 MB, 2.84% gc time)\n",
    "#   6.272144 seconds (9.54 M allocations: 574.633 MB, 2.80% gc time)\n",
    "#   6.277462 seconds (9.54 M allocations: 574.637 MB, 2.86% gc time)\n",
    "#   2.165516 seconds (2.34 M allocations: 238.323 MB, 2.56% gc time)\n",
    "# (:epoch,3,:loss,3.226540256084356)\n",
    "\n",
    "\n",
    "### SAMPLE OUTPUT (with head -10000 100.txt): first version\n",
    "# julia> CharLM.main(\"--gpu --data 10.txt\")\n",
    "# opts=(:lr,1.0)(:savefile,nothing)(:loadfile,nothing)(:dropout,0.0)(:generate,0)(:bestfile,nothing)(:embedding,256)(:gclip,5.0)(:hidden,256)(:epochs,10)(:nlayer,1)(:decay,0.9)(:gpu,true)(:seqlength,100)(:seed,42)(:batchsize,128)(:datafiles,Any[\"10.txt\"])\n",
    "# INFO: Chars read: [(\"10.txt\",425808)]\n",
    "# INFO: 87 unique chars.\n",
    "# (0,4.465127425659868)\n",
    "#   2.182693 seconds (2.36 M allocations: 240.394 MB, 1.74% gc time)\n",
    "#   7.861079 seconds (10.12 M allocations: 601.311 MB, 1.84% gc time)\n",
    "# (1,3.3244698245543285)\n",
    "#   2.159062 seconds (2.35 M allocations: 239.217 MB, 1.80% gc time)\n",
    "#   6.200085 seconds (9.55 M allocations: 575.573 MB, 2.28% gc time)\n",
    "# (2,3.24593908969621)\n",
    "#   2.352389 seconds (2.34 M allocations: 239.381 MB, 1.51% gc time)\n",
    "#   6.211946 seconds (9.55 M allocations: 575.568 MB, 2.21% gc time)\n",
    "# (3,3.226540256084356)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
