{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-Lab\n",
    "Task 7-9 are homework. Please hand seperate notebooks after\n",
    "1. Task 6 (e.g. you finished the lab)\n",
    "1. Task 7\n",
    "1. Task 8\n",
    "1. Task 9\n",
    "\n",
    "Homework is due on **October 30th**, before **8AM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Knet, AutoGrad\n",
    "using Knet: sigm_dot, tanh_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafiles    = [\"input.txt\"]  # If provided, use first file for training, second for dev, others for test.\n",
    "togenerate   = 500            # If non-zero generate given number of characters.\n",
    "epochs       = 10             # Number of epochs for training.\n",
    "hidden       = [128]          # Sizes of one or more LSTM layers.\n",
    "embed        = 168            # Size of the embedding vector.\n",
    "batchsize    = 128            # Number of sequences to train on in parallel\n",
    "seqlength    = 20             # Maximum number of steps to unroll the network for bptt. Initial epochs will use the epoch number as bptt length for faster convergence.\n",
    "seed         = -1             # Random number seed. -1 or 0 is no fixed seed\n",
    "lr           = 1e-1           # Initial learning rate\n",
    "gclip        = 3.0            # Value to clip the gradient norm at.\n",
    "dpout        = 0.0            # Dropout probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed > 0 && srand(seed)\n",
    "\n",
    "# read text and report lengths\n",
    "text = map(readstring, datafiles)\n",
    "!isempty(text) && info(\"Chars read: $(map((f,c)->(basename(f),length(c)),datafiles,text))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1: Create dictionary by completing createVocabulary function\n",
    "\n",
    " function createVocabulary takes text::Array{Any,1} that contains the names of datafiles you provided by opts[:datafiles] argument. It returns vocabulary::Dict{Char,Int}()  for given text. In this lab, your text array is length of 1. For example the text is [\"content of input\"]. Note that for the sake of simplicity, we do *NOT* use validation or test dataset in this lab. You can try it by splitting  your data into 3 different set after the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function createVocabulary(text)\n",
    "    vocab = Dict{Char,Int}()\n",
    "    # Your code starts here\n",
    "    for (char_i,unique_character) in enumerate(unique(text[1]))\n",
    "        vocab[Char(unique_character)] = char_i\n",
    "    end\n",
    "    # Your code ends here\n",
    "    return vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = createVocabulary(text)\n",
    "info(\"$(length(vocab)) unique chars.\") # The output should be 75 unique chars for input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Network funtion \n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec7.pdf\n",
    "\n",
    "function lstm is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    hsize   = size(hidden,2)\n",
    "    forget  = sigm_dot(gates[:,1:hsize])\n",
    "    ingate  = sigm_dot(gates[:,1+hsize:2hsize])\n",
    "    outgate = sigm_dot(gates[:,1+2hsize:3hsize])\n",
    "    change  = tanh_dot(gates[:,1+3hsize:end])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh_dot(cell)\n",
    "    return (hidden,cell)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2: Create Initial weights\n",
    "\n",
    " initweights creates the weights and biases for the model. We are using LSTM network. We provide init function(for weights) and bias function(for bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function initweights(hidden, vocab, embed)\n",
    "    init(d...) = xavier(d...)\n",
    "    bias(d...) = zeros(d...)\n",
    "    model = Vector{Any}(2*length(hidden)+3)\n",
    "    X = embed\n",
    "    for k = 1:length(hidden)\n",
    "        # Your code starts here\n",
    "        H = hidden[k]\n",
    "        model[2k-1] = init(X+H, 4H)\n",
    "        model[2k] = bias(1, 4H)\n",
    "        #model[2k][1:H] = 1 # forget gate bias = 1\n",
    "        X = H\n",
    "        # Your code ends here\n",
    "    end\n",
    "    model[end-2] = init(vocab,embed)\n",
    "    model[end-1] = init(hidden[end],vocab)\n",
    "    model[end] = bias(1,vocab)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-3: Create Initial state\n",
    "\n",
    " At each time step, we take the hidden state from previous time step as input. To be able to do that,first we need to initialize hidden state. We also store updated hidden states in array created here. We initialize state as a zero matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "let blank = nothing; global initstate\n",
    "    function initstate(model, batch)\n",
    "        nlayers = div(length(model)-3,2)\n",
    "        state = Vector{Any}(2*nlayers)\n",
    "        for k = 1:nlayers\n",
    "            bias = model[2k]\n",
    "            hidden = div(length(bias),4)\n",
    "            if typeof(blank)!=typeof(bias) || size(blank)!=(batch,hidden)\n",
    "                blank = fill!(similar(bias, batch, hidden),0)\n",
    "            end\n",
    "            state[2k-1] = state[2k] = blank\n",
    "        end\n",
    "        return state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-4: Create Predict function\n",
    "\n",
    " predict is a function that takes w(model) created in initweights, s(state) created in initstate and input whose size is batchsize vocabulary You need to implement predict function for LSTM. You must use lstm function here. LSTM function is provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function predict(model, state, input; pdrop=0)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    newstate = similar(state)\n",
    "    for k = 1:nlayers\n",
    "        # Your code starts here\n",
    "        input = dropout(input, pdrop)\n",
    "        (newstate[2k-1],newstate[2k]) = lstm(model[2k-1],model[2k],state[2k-1],state[2k],input)\n",
    "        input = newstate[2k-1]\n",
    "        # Your code ends here\n",
    "    end\n",
    "    return input,newstate\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Sample function\n",
    "\n",
    "Generate function is a function we use to create some text that is similar to our training data. We provide sample function to you. You can predict the next character by using sample function once you calculate the probabilities given the input. index to char is the same dictionary as you created with createdictionary function but it works in the reverse direction. It gives you the character given the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function generate(model, tok2int, nchar)\n",
    "    int2tok = Vector{Char}(length(tok2int))\n",
    "    for (k,v) in tok2int; int2tok[v] = k; end\n",
    "    input = tok2int[' ']\n",
    "    state = initstate(model, 1)\n",
    "    for t in 1:nchar\n",
    "        embed = model[end-2][[input],:]\n",
    "        ypred,state = predict(model,state,embed)\n",
    "        ypred = ypred * model[end-1] .+ model[end]\n",
    "        input = sample(exp.(logp(ypred)))\n",
    "        print(int2tok[input])\n",
    "    end\n",
    "    print(nchar)\n",
    "    println()\n",
    "end\n",
    "\n",
    "function sample(p)\n",
    "    p = convert(Array,p)\n",
    "    r = rand()\n",
    "    for c = 1:length(p)\n",
    "        r -= p[c]\n",
    "        r < 0 && return c\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, Let's generate some random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = initweights(hidden, length(vocab), embed)\n",
    "state = initstate(model,1)\n",
    "\n",
    "println(\"########## RANDOM MODEL OUTPUT ############\")\n",
    "generate(model, vocab, togenerate) ## change togenerate if you want longer sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We provide minibatch function for you. You do not have to do it for this lab. But we suggest you to understand the idea since you need to do it in your own project and future labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function minibatch(chars, tok2int, batch_size)\n",
    "    chars = collect(chars)\n",
    "    nbatch = div(length(chars), batch_size)\n",
    "    data = [zeros(Int,batch_size) for i=1:nbatch ]\n",
    "    for n = 1:nbatch\n",
    "        for b = 1:batch_size\n",
    "            char = chars[(b-1)*nbatch + n]\n",
    "            data[n][b] = tok2int[char]\n",
    "        end\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-5: Create loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be the hardest part of this lab. Implement appropriate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function loss(model, state, sequence, range=1:length(sequence)-1; newstate=nothing, pdrop=0)\n",
    "    preds = []\n",
    "    for t in range\n",
    "        input = model[end-2][sequence[t],:]\n",
    "        pred,state = predict(model,state,input; pdrop=pdrop)\n",
    "        push!(preds,pred)\n",
    "    end\n",
    "    if newstate != nothing\n",
    "        copy!(newstate, map(AutoGrad.getval,state))\n",
    "    end\n",
    "    pred0 = vcat(preds...)\n",
    "    pred1 = dropout(pred0,pdrop)\n",
    "    pred2 = pred1 * model[end-1]\n",
    "    pred3 = pred2 .+ model[end]\n",
    "    logp1 = logp(pred3,2)\n",
    "    nrows,ncols = size(pred3)\n",
    "    golds = vcat(sequence[range[1]+1:range[end]+1]...)\n",
    "    index = similar(golds)\n",
    "    @inbounds for i=1:length(golds)\n",
    "        index[i] = i + (golds[i]-1)*nrows\n",
    "    end\n",
    "    logp2 = logp1[index]\n",
    "    logp3 = sum(logp2)\n",
    "    return -logp3 / length(golds)\n",
    "end\n",
    "\n",
    "# Knet magic\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function avgloss(model, sequence, S)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    total = count = 0\n",
    "    for i in 1:S:T-1\n",
    "        j = min(i+S-1,T-1)\n",
    "        n = j-i+1\n",
    "        total += n * loss(model, state, sequence, i:j; newstate=state)\n",
    "        count += n\n",
    "    end\n",
    "    return total / count\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-6: Create Train function\n",
    "\n",
    "Implement bptt(Backpropagation through time) function for training. You need to fill up only 3 lines(or even small numbers). You need use lossgradient function and update! function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function train(model, sequence, optim, S; pdrop=0)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    for i in 1:S:T-1\n",
    "        # Your code starts here\n",
    "        j = min(i+S-1,T-1)\n",
    "        grads = lossgradient(model, state, sequence, i:j; newstate=state, pdrop=pdrop)\n",
    "        update!(model, grads, optim)\n",
    "        # Your code ends here\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready. First let's see the initial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data =  map(t->minibatch(t, vocab, batchsize), text)\n",
    "# Print the loss of randomly initialized model.\n",
    "losses = map(d->avgloss(model,d,100), data)\n",
    "println((:epoch,0,:loss,losses...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the training part of RNN(with Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim = map(x->Adam(lr=lr, gclip=gclip), model)\n",
    "# MAIN LOOP\n",
    "for epoch=1:epochs\n",
    "    @time train(model, data[1], optim, min(epoch,seqlength); pdrop=dpout)\n",
    "    # Calculate and print the losses after each epoch\n",
    "    losses = map(d->avgloss(model,d,100),data)\n",
    "    println((:epoch,epoch,:loss,losses...))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have checked the loss decreasing, let's create some text with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(\"########## FINAL  MODEL OUTPUT ############\")\n",
    "state = initstate(model,1)\n",
    "generate(model, vocab, togenerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-7 (Homework)\n",
    "\n",
    "For simplicity we have removed the GPU support from this notebook.\n",
    "What would need to change so that you can run your training on a GPU? (hin: KNetArray)\n",
    "\n",
    "Change the notebook to work on the GPU or the CPU depending on a global parameter that switches between `Array` and `KNetArray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-8 (Homework)\n",
    "Analyse the performance of this code (you might have to convert this from a notebook to a single-source file before doing so, see nbconvert)\n",
    "\n",
    "1. Read the performance tips https://docs.julialang.org/en/latest/manual/performance-tips/\n",
    "2. Use `@code_warntype` to check that you don't have type-instabilities in your code.\n",
    "3. [`ProfileView.jl`](https://github.com/timholy/ProfileView.jl) and the [profiler](https://docs.julialang.org/en/latest/manual/profile/) are your friends.\n",
    "4. Where are memory allocation happening (use the memory allocation tracker).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task-9 (Homework)\n",
    "\n",
    "In this notebook we used standard LSTM untis, but there are many variants (see http://colah.github.io/posts/2015-08-Understanding-LSTMs/):\n",
    "\n",
    "- [GRU](https://arxiv.org/pdf/1406.1078v3.pdf)\n",
    "- [peephole-LSTM](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)\n",
    "- [Depth Gated RNN](https://arxiv.org/pdf/1508.03790v2.pdf)\n",
    "- [Clockwork RNN](https://arxiv.org/pdf/1402.3511v1.pdf)\n",
    "- ...\n",
    "\n",
    "Take a look at [*Greff, et al. (2015)*](https://arxiv.org/pdf/1503.04069.pdf) and [*Jozefowicz, et al. (2015)*](http://proceedings.mlr.press/v37/jozefowicz15.pdf) and **choose** a different LSTM model and implement it.\n",
    "\n",
    "For the particular ambitious student choose a model from https://distill.pub/2016/augmented-rnns/\n",
    "\n",
    "For GRU take a look at http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
