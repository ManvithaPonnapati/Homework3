{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using ProfileView\n",
    "using Knet, AutoGrad\n",
    "using Knet: sigm_dot, tanh_dot\n",
    "# Check Task 8 notebook for profiling results Profile.init(delay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles    = [\"input.txt\"]  # If provided, use first file for training, second for dev, others for test.\n",
    "togenerate   = 500            # If non-zero generate given number of characters.\n",
    "epochs       = 10             # Number of epochs for training.\n",
    "hidden       = [128]          # Sizes of one or more LSTM layers.\n",
    "embed        = 168            # Size of the embedding vector.\n",
    "batchsize    = 128            # Number of sequences to train on in parallel\n",
    "seqlength    = 20             # Maximum number of steps to unroll the network for bptt. Initial epochs will use the epoch number as bptt length for faster convergence.\n",
    "seed         = -1             # Random number seed. -1 or 0 is no fixed seed\n",
    "lr           = 1e-1           # Initial learning rate\n",
    "gclip        = 3.0            # Value to clip the gradient norm at.\n",
    "dpout        = 0.0            # Dropout probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mChars read: Tuple{String,Int64}[(\"input.txt\", 105989)]\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "seed > 0 && srand(seed)\n",
    "\n",
    "# read text and report lengths\n",
    "text = map(readstring, datafiles)\n",
    "!isempty(text) && info(\"Chars read: $(map((f,c)->(basename(f),length(c)),datafiles,text))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-1: Create dictionary by completing createVocabulary function\n",
    "function createVocabulary takes text::Array{Any,1} that contains the names of datafiles you provided by opts[:datafiles] argument. It returns vocabulary::Dict{Char,Int}() for given text. In this lab, your text array is length of 1. For example the text is [\"content of input\"]. Note that for the sake of simplicity, we do NOT use validation or test dataset in this lab. You can try it by splitting your data into 3 different set after the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createVocabulary (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function createVocabulary(text)\n",
    "    vocab = Dict{Char,Int}()\n",
    "    # MY CODE STARTS HERE \n",
    "    \n",
    "    for (char_i,unique_character) in enumerate(unique(text[1]))\n",
    "        vocab[Char(unique_character)] = char_i\n",
    "    end\n",
    "    # MY CODE ENDS HERE\n",
    "    return vocab\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36m75 unique chars.\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "vocab = createVocabulary(text)\n",
    "info(\"$(length(vocab)) unique chars.\") # The output should be 75 unique chars for input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network function\n",
    "\n",
    "In a regular RNN - The core idea is to use past hidden weights, present input to calculate the next set of hidden state weights. i.e st = (Ux/t + W/st-1) . \n",
    "\n",
    "LSTM equations looks scary and there are a lot of them forget gate, ingate , output gate and change . But it's basically just another way to calculate the hidden states (except in this scenario the vanishing gradients is not an issue). \n",
    "\n",
    "In a LSTM network you have initialize four sets of weights Wf (Forget gate ft) Wi - input gage, Wo - Output, Wc - Change gate weights and the corresponding biases. So instead of creating new variables looks like this cell code is just creating one giant weights/gates and then split it into parts of equal width(columnwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    hsize   = size(hidden,2)\n",
    "    forget  = sigm_dot(gates[:,1:hsize])\n",
    "    ingate  = sigm_dot(gates[:,1+hsize:2hsize])\n",
    "    outgate = sigm_dot(gates[:,1+2hsize:3hsize])\n",
    "    change  = tanh_dot(gates[:,1+3hsize:end])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh_dot(cell)\n",
    "    return (hidden,cell)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-2: Create Initial weights\n",
    "initweights creates the weights and biases for the model. We are using LSTM network. We provide init function(for weights) and bias function(for bias)\n",
    "\n",
    "First we have to initialize weights from the placeholder here x=embed and embed being 168 looks like the input vector is 168 \n",
    "long vector. The first part of this model is the relationshp betwen concatted inputs and hidden weights .This belongs to the embedding layer. Also for multiple hidden layers, y1=cell(x) , y2= cell(y1).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(hidden, vocab, embed)\n",
    "    init(d...) = xavier(d...)\n",
    "    bias(d...) = zeros(d...)\n",
    "    model = Vector{Any}(2*length(hidden)+3)\n",
    "    X = embed\n",
    "    for k = 1:length(hidden)\n",
    "        # MY CODE STARTS HERE\n",
    "        #Concatted input and hidden layer weights\n",
    "        num_nodes_hidden = hidden[k]\n",
    "        model[2k-1]=init(X+num_nodes_hidden,4*num_nodes_hidden) #Because we have to initialize 4w's - wf,wi,wo,wc\n",
    "        model[2k]=bias(1,4*num_nodes_hidden)\n",
    "        X = num_nodes_hidden\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "    model[end-2] = init(vocab,embed)\n",
    "    model[end-1] = init(hidden[end],vocab)\n",
    "    model[end] = bias(1,vocab)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-3: Create Initial state\n",
    "At each time step, we take the hidden state from previous time step as input. To be able to do that,first we need to initialize hidden state. We also store updated hidden states in array created here. We initialize state as a zero matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initstate (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let blank = nothing; global initstate\n",
    "    function initstate(model, batch)\n",
    "        nlayers = div(length(model)-3,2)\n",
    "        state = Vector{Any}(2*nlayers)\n",
    "        for k = 1:nlayers\n",
    "            bias = model[2k]\n",
    "            hidden = div(length(bias),4)\n",
    "            if typeof(blank)!=typeof(bias) || size(blank)!=(batch,hidden)\n",
    "                blank = fill!(similar(bias, batch, hidden),0)\n",
    "            end\n",
    "            state[2k-1] = state[2k] = blank\n",
    "        end\n",
    "        return state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-4: Create Predict function\n",
    "predict is a function that takes w(model) created in initweights, s(state) created in initstate and input whose size is batchsize vocabulary You need to implement predict function for LSTM. You must use lstm function here. LSTM function is provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(model, state, input; pdrop=0)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    newstate = similar(state)\n",
    "    for k = 1:nlayers\n",
    "        # MY CODE STARTS HERE\n",
    "        #newstate[2k-1] is the hidden layer (look at the initweights for more explanation)\n",
    "        input = dropout(input, pdrop)\n",
    "        (newstate[2k-1],newstate[2k])=lstm(model[2k-1],model[2k],state[2k-1],state[2k],input)\n",
    "        input = newstate[2k-1]\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "    return input,newstate\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Sample function\n",
    "Generate function is a function we use to create some text that is similar to our training data. We provide sample function to you. You can predict the next character by using sample function once you calculate the probabilities given the input. index to char is the same dictionary as you created with createdictionary function but it works in the reverse direction. It gives you the character given the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate(model, tok2int, nchar)\n",
    "    int2tok = Vector{Char}(length(tok2int))\n",
    "    for (k,v) in tok2int; int2tok[v] = k; end\n",
    "    input = tok2int[' ']\n",
    "    state = initstate(model, 1)\n",
    "    for t in 1:nchar\n",
    "        embed = model[end-2][[input],:]\n",
    "        ypred,state = predict(model,state,embed)\n",
    "        ypred = ypred * model[end-1] .+ model[end]\n",
    "        input = sample(exp.(logp(ypred)))\n",
    "        print(int2tok[input])\n",
    "    end\n",
    "    println()\n",
    "end\n",
    "\n",
    "function sample(p)\n",
    "    p = convert(Array,p)\n",
    "    r = rand()\n",
    "    for c = 1:length(p)\n",
    "        r -= p[c]\n",
    "        r < 0 && return c\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, Let's generate some random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## RANDOM MODEL OUTPUT ############\n",
      "9SdL2oK2qaxnd&!k3hhrq:Ph'KAbR&}mV;HqL5P\\8r}CqRDwNu',odc&HnYVA-TKVaRM\n",
      "5\n",
      "5TcTc*IJrTBFfD vPU2ktKYH\\aFzP-\\-4YG{6iECRy4S\\sw8*RFhggo8I:mxMm8S-sm.&w9265ka03FA\n",
      "!*}:1rA2rJDwUsJ9D1Jax&riWh13cztPt;cnfhn? Q1VF3PvYnRIU 5KSl3e37k,BEG}a6\\d.9BfE:e'!i:ATfgkGt\n",
      "g-8?Jac5D}a\\:IkAJ9B9fgt9ep&Fvkj-?8dNP44ffJ:fQ?EJL*91tmoxN&nI\n",
      "R8.s-hS2\\SEywliCn&-'8Do7:IfVLDQim?RLqrRB*yM5:W*Jbo95hSdVwpaUuKON}kAeEW9DLhM!JB\\q3jMlj TWNd:HUUdH3yoY-!JiqA.58phrp&7E0MBac?Qt&3v-pkm\n",
      "nG&HRnNr*HN0SHOJiiMOEcr58UGxu4:v4g0J8!9K?x\n",
      "4nHxT,JU':YGUo-IRy2ou\n"
     ]
    }
   ],
   "source": [
    "model = initweights(hidden, length(vocab), embed)\n",
    "state = initstate(model,1)\n",
    "\n",
    "println(\"########## RANDOM MODEL OUTPUT ############\")\n",
    "generate(model, vocab, togenerate) ## change togenerate if you want longer sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide minibatch function for you. You do not have to do it for this lab. But we suggest you to understand the idea since you need to do it in your own project and future labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function minibatch(chars, tok2int, batch_size)\n",
    "    chars = collect(chars)\n",
    "    nbatch = div(length(chars), batch_size)\n",
    "    data = [zeros(Int,batch_size) for i=1:nbatch ]\n",
    "    for n = 1:nbatch\n",
    "        for b = 1:batch_size\n",
    "            char = chars[(b-1)*nbatch + n]\n",
    "            data[n][b] = tok2int[char]\n",
    "        end\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-5: Create loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgloss (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, state, sequence, range=1:length(sequence)-1; newstate=nothing, pdrop=0)\n",
    "    preds = []\n",
    "    for t in range\n",
    "        input = model[end-2][sequence[t],:]\n",
    "        pred,state = predict(model,state,input; pdrop=pdrop)\n",
    "        push!(preds,pred)\n",
    "    end\n",
    "    if newstate != nothing\n",
    "        copy!(newstate, map(AutoGrad.getval,state))\n",
    "    end\n",
    "    pred0 = vcat(preds...)\n",
    "    pred1 = dropout(pred0,pdrop)\n",
    "    pred2 = pred1 * model[end-1]\n",
    "    pred3 = pred2 .+ model[end]\n",
    "    logp1 = logp(pred3,2)\n",
    "    nrows,ncols = size(pred3)\n",
    "    golds = vcat(sequence[range[1]+1:range[end]+1]...)\n",
    "    index = similar(golds)\n",
    "    @inbounds for i=1:length(golds)\n",
    "        index[i] = i + (golds[i]-1)*nrows\n",
    "    end\n",
    "    logp2 = logp1[index]\n",
    "    logp3 = sum(logp2)\n",
    "    return -logp3 / length(golds)\n",
    "end\n",
    "\n",
    "# Knet magic\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function avgloss(model, sequence, S)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    total = count = 0\n",
    "    for i in 1:S:T-1\n",
    "        j = min(i+S-1,T-1)\n",
    "        n = j-i+1\n",
    "        total += n * loss(model, state, sequence, i:j; newstate=state)\n",
    "        count += n\n",
    "    end\n",
    "    return total / count\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-6: Create Train function¶\n",
    "Implement bptt(Backpropagation through time) function for training. You need to fill up only 3 lines(or even small numbers). You need use lossgradient function and update! function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(model, sequence, optim, S; pdrop=0)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    for i in 1:S:T-1\n",
    "        # MY CODE STARTS HERE\n",
    "        end_seq = 1+S-1\n",
    "        if end_seq > T-1\n",
    "            end_seq = T-1\n",
    "        end\n",
    "        gradient_loss = lossgradient(model,state,sequence,1:end_seq,newstate=state,pdrop=pdrop)\n",
    "        update!(model,gradient_loss,optim)\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are ready. First let's see the initial loss¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 0, :loss, 4.317637547848029)\n"
     ]
    }
   ],
   "source": [
    "data =  map(t->minibatch(t, vocab, batchsize), text)\n",
    "# Print the loss of randomly initialized model.\n",
    "losses = map(d->avgloss(model,d,100), data)\n",
    "println((:epoch,0,:loss,losses...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the training part of RNN(with Adam)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16.154698 seconds (3.76 M allocations: 13.204 GiB, 9.28% gc time)\n",
      "(:epoch, 1, :loss, 6.4339138128636755)\n",
      " 26.365740 seconds (1.82 M allocations: 10.573 GiB, 5.04% gc time)\n",
      "(:epoch, 2, :loss, 16.779430804354813)\n",
      " 27.217983 seconds (1.63 M allocations: 9.716 GiB, 5.92% gc time)\n",
      "(:epoch, 3, :loss, 17.627699673150754)\n",
      " 25.884419 seconds (1.56 M allocations: 9.288 GiB, 7.03% gc time)\n",
      "(:epoch, 4, :loss, 15.686559137198309)\n",
      " 30.129568 seconds (1.56 M allocations: 9.053 GiB, 18.97% gc time)\n",
      "(:epoch, 5, :loss, 14.844173007429832)\n",
      " 26.656316 seconds (1.56 M allocations: 8.861 GiB, 16.26% gc time)\n",
      "(:epoch, 6, :loss, 13.489816137159679)\n",
      " 28.251888 seconds (1.58 M allocations: 8.792 GiB, 21.83% gc time)\n",
      "(:epoch, 7, :loss, 12.161539188062907)\n",
      " 33.450087 seconds (1.58 M allocations: 8.689 GiB, 24.93% gc time)\n",
      "(:epoch, 8, :loss, 10.425839088980421)\n",
      " 32.550340 seconds (1.58 M allocations: 8.576 GiB, 26.77% gc time)\n",
      "(:epoch, 9, :loss, 9.403993777537233)\n",
      " 31.899722 seconds (1.58 M allocations: 8.540 GiB, 24.80% gc time)\n",
      "(:epoch, 10, :loss, 8.654967191827083)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: clear_malloc_data not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: clear_malloc_data not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "optim = map(x->Adam(lr=lr, gclip=gclip), model)\n",
    "# MAIN LOOP\n",
    "function trainingloop()\n",
    "    for epoch=1:epochs\n",
    "        @time train(model, data[1], optim, min(epoch,seqlength); pdrop=dpout)\n",
    "        # Calculate and print the losses after each epoch\n",
    "        losses = map(d->avgloss(model,d,100),data)\n",
    "        println((:epoch,epoch,:loss,losses...))\n",
    "    end\n",
    "end\n",
    "trainingloop()\n",
    "Profile.clear_malloc_data()\n",
    "trainingloop()\n",
    "# Profile.clear()\n",
    "# @profile trainingloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you have checked the loss decreasing, let's create some text with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## FINAL  MODEL OUTPUT ############\n",
      "Variables:\n",
      "  #self#::#generate\n",
      "  model::Array{Any,1}\n",
      "  tok2int::Dict{Char,Int64}\n",
      "  nchar::Int64\n",
      "  k::Char\n",
      "  v::Int64\n",
      "  #temp#@_7::Int64\n",
      "  #temp#@_8::Int64\n",
      "  t::Int64\n",
      "  embed\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m\n",
      "  #temp#@_11::Int64\n",
      "  ypred\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m\n",
      "  #temp#@_13::Int64\n",
      "  int2tok::Array{Char,1}\n",
      "  input\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m\n",
      "  state::Array{Any,1}\n",
      "  i::Int64\n",
      "  index::Int64\n",
      "  #temp#@_19::Int64\n",
      "\n",
      "Body:\n",
      "  begin \n",
      "      NewvarNode(:(input\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m))\n",
      "      NewvarNode(:(state::Array{Any,1}))\n",
      "      SSAValue(11) = (Core.getfield)(tok2int::Dict{Char,Int64}, :count)::Int64\n",
      "      int2tok::Array{Char,1} = $(Expr(:foreigncall, :(:jl_alloc_array_1d), Array{Char,1}, svec(Any, Int64), Array{Char,1}, 0, SSAValue(11), 0)) # line 3:\n",
      "      $(Expr(:inbounds, false))\n",
      "      # meta: location dict.jl start 574\n",
      "      i::Int64 = $(Expr(:invoke, MethodInstance for skip_deleted(::Dict{Char,Int64}, ::Int64), :(Base.skip_deleted), :(tok2int), :((Core.getfield)(tok2int, :idxfloor)::Int64))) # line 575:\n",
      "      (Core.setfield!)(tok2int::Dict{Char,Int64}, :idxfloor, i::Int64)::Int64\n",
      "      # meta: pop location\n",
      "      $(Expr(:inbounds, :pop))\n",
      "      #temp#@_8::Int64 = i::Int64\n",
      "      14: \n",
      "      unless (Base.not_int)((Base.slt_int)((Base.arraylen)((Core.getfield)(tok2int::Dict{Char,Int64}, :vals)::Array{Int64,1})::Int64, #temp#@_8::Int64)::Bool)::Bool goto 30\n",
      "      SSAValue(24) = $(Expr(:new, Pair{Char,Int64}, :((Base.arrayref)((Core.getfield)(tok2int, :keys)::Array{Char,1}, #temp#@_8)::Char), :((Base.arrayref)((Core.getfield)(tok2int, :vals)::Array{Int64,1}, #temp#@_8)::Int64)))\n",
      "      SSAValue(25) = $(Expr(:invoke, MethodInstance for skip_deleted(::Dict{Char,Int64}, ::Int64), :(Base.skip_deleted), :(tok2int), :((Base.add_int)(#temp#@_8, 1)::Int64)))\n",
      "      SSAValue(2) = SSAValue(24)\n",
      "      SSAValue(26) = (Base.getfield)(SSAValue(2), 1)::Char\n",
      "      SSAValue(27) = (Base.add_int)(1, 1)::Int64\n",
      "      k::Char = SSAValue(26)\n",
      "      SSAValue(28) = (Base.getfield)(SSAValue(2), 2)::Int64\n",
      "      SSAValue(29) = (Base.add_int)(2, 1)::Int64\n",
      "      v::Int64 = SSAValue(28)\n",
      "      #temp#@_8::Int64 = SSAValue(25) # line 3:\n",
      "      (Base.arrayset)(int2tok::Array{Char,1}, k::Char, v::Int64)::Array{Char,1}\n",
      "      28: \n",
      "      goto 14\n",
      "      30:  # line 4:\n",
      "      $(Expr(:inbounds, false))\n",
      "      # meta: location dict.jl getindex 473\n",
      "      index::Int64 = $(Expr(:invoke, MethodInstance for ht_keyindex(::Dict{Char,Int64}, ::Char), :(Base.ht_keyindex), :(tok2int), ' ')) # line 474:\n",
      "      unless (Base.slt_int)(index::Int64, 0)::Bool goto 39\n",
      "      #temp#@_19::Int64 = (Base.throw)($(Expr(:new, :(Base.KeyError), ' ')))\u001b[1m\u001b[91m::Union{}\u001b[39m\u001b[22m\n",
      "      goto 41\n",
      "      39: \n",
      "      #temp#@_19::Int64 = (Base.arrayref)((Core.getfield)(tok2int::Dict{Char,Int64}, :vals)::Array{Int64,1}, index::Int64)::Int64\n",
      "      41: \n",
      "      # meta: pop location\n",
      "      $(Expr(:inbounds, :pop))\n",
      "      input\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m = #temp#@_19::Int64 # line 5:\n",
      "      state::Array{Any,1} = $(Expr(:invoke, MethodInstance for initstate(::Array{Any,1}, ::Int64), :(Main.initstate), :(model), 1)) # line 6:\n",
      "      SSAValue(30) = (Base.select_value)((Base.sle_int)(1, nchar::Int64)::Bool, nchar::Int64, (Base.sub_int)(1, 1)::Int64)::Int64\n",
      "      #temp#@_13::Int64 = 1\n",
      "      50: \n",
      "      unless (Base.not_int)((#temp#@_13::Int64 === (Base.add_int)(SSAValue(30), 1)::Int64)::Bool)::Bool goto 108\n",
      "      SSAValue(31) = #temp#@_13::Int64\n",
      "      SSAValue(32) = (Base.add_int)(#temp#@_13::Int64, 1)::Int64\n",
      "      #temp#@_13::Int64 = SSAValue(32) # line 7:\n",
      "      $(Expr(:inbounds, false))\n",
      "      # meta: location abstractarray.jl endof 134\n",
      "      # meta: location abstractarray.jl linearindices 99\n",
      "      # meta: location abstractarray.jl indices1 71\n",
      "      # meta: location abstractarray.jl indices 64\n",
      "      SSAValue(15) = (Base.arraysize)(model::Array{Any,1}, 1)::Int64\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      $(Expr(:inbounds, :pop))\n",
      "      SSAValue(7) = (Base.arrayref)(model::Array{Any,1}, (Base.sub_int)((Base.select_value)((Base.slt_int)(SSAValue(15), 0)::Bool, 0, SSAValue(15))::Int64, 2)::Int64)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m\n",
      "      embed\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m = (Main.getindex)(SSAValue(7), (Base.vect)(input\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Array{_,1} where _\u001b[39m\u001b[22m, Main.:)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m # line 8:\n",
      "      SSAValue(8) = (Main.#predict#10)(0, Main.predict, model::Array{Any,1}, state::Array{Any,1}, embed\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Tuple{Any,Array{Any,1}}\u001b[39m\u001b[22m\n",
      "      SSAValue(33) = (Base.getfield)(SSAValue(8), 1)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m\n",
      "      SSAValue(34) = (Base.add_int)(1, 1)::Int64\n",
      "      ypred\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m = SSAValue(33)\n",
      "      SSAValue(35) = (Base.getfield)(SSAValue(8), 2)::Array{Any,1}\n",
      "      SSAValue(36) = (Base.add_int)(2, 1)::Int64\n",
      "      state::Array{Any,1} = SSAValue(35) # line 9:\n",
      "      $(Expr(:inbounds, false))\n",
      "      # meta: location abstractarray.jl endof 134\n",
      "      # meta: location abstractarray.jl linearindices 99\n",
      "      # meta: location abstractarray.jl indices1 71\n",
      "      # meta: location abstractarray.jl indices 64\n",
      "      SSAValue(21) = (Base.arraysize)(model::Array{Any,1}, 1)::Int64\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      $(Expr(:inbounds, :pop))\n",
      "      SSAValue(22) = (ypred\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m * (Base.arrayref)(model::Array{Any,1}, (Base.sub_int)((Base.select_value)((Base.slt_int)(SSAValue(21), 0)::Bool, 0, SSAValue(21))::Int64, 1)::Int64)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m\n",
      "      $(Expr(:inbounds, false))\n",
      "      # meta: location abstractarray.jl endof 134\n",
      "      # meta: location abstractarray.jl linearindices 99\n",
      "      # meta: location abstractarray.jl indices1 71\n",
      "      # meta: location abstractarray.jl indices 64\n",
      "      SSAValue(18) = (Base.arraysize)(model::Array{Any,1}, 1)::Int64\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      $(Expr(:inbounds, :pop))\n",
      "      ypred\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m = (Base.broadcast)(Main.+, SSAValue(22), (Base.arrayref)(model::Array{Any,1}, (Base.select_value)((Base.slt_int)(SSAValue(18), 0)::Bool, 0, SSAValue(18))::Int64)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m # line 10:\n",
      "      input\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m = (Main.sample)((Base.broadcast)(Main.exp, (Main.logp)(ypred\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m # line 11:\n",
      "      (Main.print)((Main.getindex)(int2tok::Array{Char,1}, input\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m)\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m\n",
      "      106: \n",
      "      goto 50\n",
      "      108:  # line 13:\n",
      "      $(Expr(:inbounds, false))\n",
      "      # meta: location coreio.jl println 5\n",
      "      SSAValue(23) = (Core.typeassert)(Base.STDOUT, Base.IO)\u001b[1m\u001b[91m::IO\u001b[39m\u001b[22m\n",
      "      # meta: location coreio.jl println 6\n",
      "      # meta: location char.jl print 45\n",
      "      (Base.write)(SSAValue(23), '\\n')\u001b[1m\u001b[91m::Any\u001b[39m\u001b[22m\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      # meta: pop location\n",
      "      $(Expr(:inbounds, :pop))\n",
      "      return Base.nothing\n",
      "  end::Void\n"
     ]
    }
   ],
   "source": [
    "println(\"########## FINAL  MODEL OUTPUT ############\")\n",
    "state = initstate(model,1)\n",
    "generate(model, vocab, togenerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mThe profile data buffer is full; profiling probably terminated\n",
      "before your program finished. To profile for longer runs, call Profile.init\n",
      "with a larger buffer and/or larger delay.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# open(\"cpu_profile.bin\", \"w\") do f serialize(f, Profile.retrieve()) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
