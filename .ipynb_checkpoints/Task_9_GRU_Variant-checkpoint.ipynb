{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array{Float32,N} where N"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ProfileView\n",
    "using Knet, AutoGrad\n",
    "using Knet: sigm_dot, tanh_dot\n",
    "Profile.init(delay=0.01)\n",
    "\n",
    "#type_of_array = eval(parse(\"KnetArray{Float32}\"))  #TO ENABLE GPU\n",
    "type_of_array = eval(parse(\"Array{Float32}\"))  #TO USE CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles    = [\"input.txt\"]  # If provided, use first file for training, second for dev, others for test.\n",
    "togenerate   = 500            # If non-zero generate given number of characters.\n",
    "epochs       = 20            # Number of epochs for training.\n",
    "hidden       = [128]          # Sizes of one or more LSTM layers.\n",
    "embed        = 168            # Size of the embedding vector.\n",
    "batchsize    = 128            # Number of sequences to train on in parallel\n",
    "seqlength    = 20             # Maximum number of steps to unroll the network for bptt. Initial epochs will use the epoch number as bptt length for faster convergence.\n",
    "seed         = -1             # Random number seed. -1 or 0 is no fixed seed\n",
    "lr           = 1e-1           # Initial learning rate\n",
    "gclip        = 3.0            # Value to clip the gradient norm at.\n",
    "dpout        = 0.0            # Dropout probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mChars read: Tuple{String,Int64}[(\"input.txt\", 105989)]\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "seed > 0 && srand(seed)\n",
    "\n",
    "# read text and report lengths\n",
    "text = map(readstring, datafiles)\n",
    "!isempty(text) && info(\"Chars read: $(map((f,c)->(basename(f),length(c)),datafiles,text))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-1: Create dictionary by completing createVocabulary function\n",
    "function createVocabulary takes text::Array{Any,1} that contains the names of datafiles you provided by opts[:datafiles] argument. It returns vocabulary::Dict{Char,Int}() for given text. In this lab, your text array is length of 1. For example the text is [\"content of input\"]. Note that for the sake of simplicity, we do NOT use validation or test dataset in this lab. You can try it by splitting your data into 3 different set after the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createVocabulary (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function createVocabulary(text)\n",
    "    vocab = Dict{Char,Int}()\n",
    "    # MY CODE STARTS HERE \n",
    "    \n",
    "    for (char_i,unique_character) in enumerate(unique(text[1]))\n",
    "        vocab[Char(unique_character)] = char_i\n",
    "    end\n",
    "    # MY CODE ENDS HERE\n",
    "    return vocab\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36m75 unique chars.\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "vocab = createVocabulary(text)\n",
    "info(\"$(length(vocab)) unique chars.\") # The output should be 75 unique chars for input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Gates/Cell function\n",
    "\n",
    "In a regular RNN - The core idea is to use past hidden weights, present input to calculate the next set of hidden state weights. i.e st = (Ux/t + W/st-1) . \n",
    "\n",
    "\n",
    "In a GRU network the big difference is actually in the RNN cell. Instead of Wf,Wi,Wc as in LSTM from earlier, we instead use the reset gate rt and update gate zt .  So instead of three gates in the LSTM network we instead only have two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    hsize   = size(hidden,2)\n",
    "    zt  = sigm_dot(hcat(input,hidden) *weight[:,1:hsize] .+ bias[:,1:hsize])\n",
    "    rt  = sigm_dot(hcat(input,hidden) *weight[:,1+hsize:2hsize] .+ bias[:,1+hsize:2hsize])\n",
    "    change  = tanh_dot(hcat(input,rt.*hidden) *weight[:,1+2hsize:3hsize] .+ bias[:,1+2hsize:3hsize])\n",
    "    hidden  = zt .* hidden + (1-zt).*change\n",
    "    cell = hidden #Because GRU merges cell state and hidden state Ref: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "    return (hidden,cell)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-2: Create Initial weights\n",
    "initweights creates the weights and biases for the model. We are using GRU network. We provide init function(for weights) and bias function(for bias)\n",
    "\n",
    "First we have to initialize weights from the placeholder here x=embed and embed being 168 looks like the input vector is 168 \n",
    "long vector. The first part of this model is the relationshp betwen concatted inputs and hidden weights .This belongs to the embedding layer. Also for multiple hidden layers, y1=cell(x) , y2= cell(y1).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(hidden, vocab, embed)\n",
    "    init(d...) = type_of_array(xavier(d...))\n",
    "    bias(d...) = type_of_array(zeros(d...))\n",
    "    model = Vector{Any}(2*length(hidden)+3)\n",
    "    X = embed\n",
    "    for k = 1:length(hidden)\n",
    "        # MY CODE STARTS HERE\n",
    "        #Concatted input and hidden layer weights\n",
    "        num_nodes_hidden = hidden[k]\n",
    "        model[2k-1]=init(X+num_nodes_hidden,4*num_nodes_hidden) #Because we have to initialize 4w's - wf,wi,wo,wc\n",
    "        model[2k]=bias(1,4*num_nodes_hidden)\n",
    "        X = num_nodes_hidden\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "    model[end-2] = init(vocab,embed)\n",
    "    model[end-1] = init(hidden[end],vocab)\n",
    "    model[end] = bias(1,vocab)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-3: Create Initial state\n",
    "At each time step, we take the hidden state from previous time step as input. To be able to do that,first we need to initialize hidden state. We also store updated hidden states in array created here. We initialize state as a zero matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initstate (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let blank = nothing; global initstate\n",
    "    function initstate(model, batch)\n",
    "        nlayers = div(length(model)-3,2)\n",
    "        state = Vector{Any}(2*nlayers)\n",
    "        for k = 1:nlayers\n",
    "            bias = model[2k]\n",
    "            hidden = div(length(bias),4)\n",
    "            if typeof(blank)!=typeof(bias) || size(blank)!=(batch,hidden)\n",
    "                blank = fill!(similar(bias, batch, hidden),0)\n",
    "            end\n",
    "            state[2k-1] = state[2k] = blank\n",
    "        end\n",
    "        return state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-4: Create Predict function\n",
    "predict is a function that takes w(model) created in initweights, s(state) created in initstate and input whose size is batchsize vocabulary You need to implement predict function for LSTM. You must use lstm function here. LSTM function is provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(model, state, input; pdrop=0)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    newstate = similar(state)\n",
    "    for k = 1:nlayers\n",
    "        # MY CODE STARTS HERE\n",
    "        #newstate[2k-1] is the hidden layer (look at the initweights for more explanation)\n",
    "        input = dropout(input, pdrop)\n",
    "        (newstate[2k-1],newstate[2k])=lstm(model[2k-1],model[2k],state[2k-1],state[2k],input)\n",
    "        input = newstate[2k-1]\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "    return input,newstate\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Sample function\n",
    "Generate function is a function we use to create some text that is similar to our training data. We provide sample function to you. You can predict the next character by using sample function once you calculate the probabilities given the input. index to char is the same dictionary as you created with createdictionary function but it works in the reverse direction. It gives you the character given the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate(model, tok2int, nchar)\n",
    "    int2tok = Vector{Char}(length(tok2int))\n",
    "    for (k,v) in tok2int; int2tok[v] = k; end\n",
    "    input = tok2int[' ']\n",
    "    state = initstate(model, 1)\n",
    "    for t in 1:nchar\n",
    "        embed = model[end-2][[input],:]\n",
    "        ypred,state = predict(model,state,embed)\n",
    "        ypred = ypred * model[end-1] .+ model[end]\n",
    "        input = sample(exp.(logp(ypred)))\n",
    "        print(int2tok[input])\n",
    "    end\n",
    "    println()\n",
    "end\n",
    "\n",
    "function sample(p)\n",
    "    p = convert(Array,p)\n",
    "    r = rand()\n",
    "    for c = 1:length(p)\n",
    "        r -= p[c]\n",
    "        r < 0 && return c\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, Let's generate some random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## RANDOM MODEL OUTPUT ############\n",
      "a-'U9EDViQN9' -UlmF1BRC8D}uRTGP4h4sa2Lq.GT1z;4JwL7iH;VTjvHonFkc!wihwnQTt07A?YiQowSUoYVNG-{Bp9CVxGeB9,I1z!Ff.3Q!lcdhSs{dKSMtUR1uKs08g-fGHW'Sel:e*t8D!R&ob798?Pe7glsgl0MeuSc:Meq3'oieqd9RUbNwbC'Reeh2uVgHif{D'F'ctgSk?ky8V;ETukU95A'k&75eT0j*T\n",
      "IsC'mo&KVS7KziDPkVzBCzOxpYoH!zS\n",
      "22:7G;3Hix1'PsKgl}Oz8vBf'9jTnw;ujkJSn;LvensY*nABwGQ4UxUVVv9vfeY,'7t6;{WIflIAxVGlgDfPi4L8hhhc0L4ebAhlG5O&5?8;W*KSw4eg{j&CgxzVvi!{gWGDnOiOq-M1\n",
      "Y!:SCKTcTB?I9rwbhWm8Ps:LryMSL--aSdRetm8{N8i0\n",
      "6:G,6t6VnDTzDGNqcfIU4;UmfPQgJdVG4'\\ S{v4x'p3g\n"
     ]
    }
   ],
   "source": [
    "model = initweights(hidden, length(vocab), embed)\n",
    "state = initstate(model,1)\n",
    "\n",
    "println(\"########## RANDOM MODEL OUTPUT ############\")\n",
    "generate(model, vocab, togenerate) ## change togenerate if you want longer sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide minibatch function for you. You do not have to do it for this lab. But we suggest you to understand the idea since you need to do it in your own project and future labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function minibatch(chars, tok2int, batch_size)\n",
    "    chars = collect(chars)\n",
    "    nbatch = div(length(chars), batch_size)\n",
    "    data = [zeros(Int,batch_size) for i=1:nbatch ]\n",
    "    for n = 1:nbatch\n",
    "        for b = 1:batch_size\n",
    "            char = chars[(b-1)*nbatch + n]\n",
    "            data[n][b] = tok2int[char]\n",
    "        end\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-5: Create loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgloss (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, state, sequence, range=1:length(sequence)-1; newstate=nothing, pdrop=0)\n",
    "    preds = []\n",
    "    for t in range\n",
    "        input = model[end-2][sequence[t],:]\n",
    "        pred,state = predict(model,state,input; pdrop=pdrop)\n",
    "        push!(preds,pred)\n",
    "    end\n",
    "    if newstate != nothing\n",
    "        copy!(newstate, map(AutoGrad.getval,state))\n",
    "    end\n",
    "    pred0 = vcat(preds...)\n",
    "    pred1 = dropout(pred0,pdrop)\n",
    "    pred2 = pred1 * model[end-1]\n",
    "    pred3 = pred2 .+ model[end]\n",
    "    logp1 = logp(pred3,2)\n",
    "    nrows,ncols = size(pred3)\n",
    "    golds = vcat(sequence[range[1]+1:range[end]+1]...)\n",
    "    index = similar(golds)\n",
    "    @inbounds for i=1:length(golds)\n",
    "        index[i] = i + (golds[i]-1)*nrows\n",
    "    end\n",
    "    logp2 = logp1[index]\n",
    "    logp3 = sum(logp2)\n",
    "    return -logp3 / length(golds)\n",
    "end\n",
    "\n",
    "# Knet magic\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function avgloss(model, sequence, S)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    total = count = 0\n",
    "    for i in 1:S:T-1\n",
    "        j = min(i+S-1,T-1)\n",
    "        n = j-i+1\n",
    "        total += n * loss(model, state, sequence, i:j; newstate=state)\n",
    "        count += n\n",
    "    end\n",
    "    return total / count\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-6: Create Train function¶\n",
    "Implement bptt(Backpropagation through time) function for training. You need to fill up only 3 lines(or even small numbers). You need use lossgradient function and update! function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(model, sequence, optim, S; pdrop=0)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    for i in 1:S:T-1\n",
    "        # MY CODE STARTS HERE\n",
    "        end_seq = 1+S-1\n",
    "        if end_seq > T-1\n",
    "            end_seq = T-1\n",
    "        end\n",
    "        gradient_loss = lossgradient(model,state,sequence,1:end_seq,newstate=state,pdrop=pdrop)\n",
    "        update!(model,gradient_loss,optim)\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are ready. First let's see the initial loss¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 0, :loss, 4.3176475f0)\n"
     ]
    }
   ],
   "source": [
    "data =  map(t->minibatch(t, vocab, batchsize), text)\n",
    "# Print the loss of randomly initialized model.\n",
    "losses = map(d->avgloss(model,d,100), data)\n",
    "println((:epoch,0,:loss,losses...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the training part of RNN(with Adam)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14.939984 seconds (4.30 M allocations: 10.176 GiB, 6.88% gc time)\n",
      "(:epoch, 1, :loss, 10.933669f0)\n",
      " 10.226540 seconds (2.21 M allocations: 7.566 GiB, 8.03% gc time)\n",
      "(:epoch, 2, :loss, 26.944506f0)\n",
      " 12.466976 seconds (2.02 M allocations: 6.718 GiB, 7.10% gc time)\n",
      "(:epoch, 3, :loss, 30.272936f0)\n",
      "  9.648352 seconds (1.94 M allocations: 6.295 GiB, 8.90% gc time)\n",
      "(:epoch, 4, :loss, 26.522581f0)\n",
      "  9.292732 seconds (1.93 M allocations: 6.056 GiB, 10.24% gc time)\n",
      "(:epoch, 5, :loss, 25.726688f0)\n",
      " 10.722867 seconds (1.93 M allocations: 5.872 GiB, 10.97% gc time)\n",
      "(:epoch, 6, :loss, 23.462496f0)\n",
      " 10.787516 seconds (1.95 M allocations: 5.787 GiB, 10.77% gc time)\n",
      "(:epoch, 7, :loss, 21.65705f0)\n",
      " 11.472688 seconds (1.95 M allocations: 5.689 GiB, 21.19% gc time)\n",
      "(:epoch, 8, :loss, 20.116842f0)\n",
      " 11.544814 seconds (1.94 M allocations: 5.591 GiB, 23.45% gc time)\n",
      "(:epoch, 9, :loss, 18.059036f0)\n",
      " 12.759594 seconds (1.95 M allocations: 5.548 GiB, 30.74% gc time)\n",
      "(:epoch, 10, :loss, 16.714403f0)\n",
      " 12.771214 seconds (1.96 M allocations: 5.542 GiB, 36.18% gc time)\n",
      "(:epoch, 11, :loss, 14.848607f0)\n",
      " 13.611179 seconds (1.94 M allocations: 5.451 GiB, 40.61% gc time)\n",
      "(:epoch, 12, :loss, 14.284809f0)\n",
      " 16.453888 seconds (1.95 M allocations: 5.444 GiB, 41.24% gc time)\n",
      "(:epoch, 13, :loss, 13.263503f0)\n",
      " 14.430282 seconds (1.99 M allocations: 5.469 GiB, 39.93% gc time)\n",
      "(:epoch, 14, :loss, 12.164851f0)\n",
      " 14.814353 seconds (1.98 M allocations: 5.444 GiB, 44.53% gc time)\n",
      "(:epoch, 15, :loss, 10.918258f0)\n",
      " 12.200880 seconds (1.95 M allocations: 5.371 GiB, 43.04% gc time)\n",
      "(:epoch, 16, :loss, 9.9970665f0)\n",
      " 12.860323 seconds (1.96 M allocations: 5.358 GiB, 42.40% gc time)\n",
      "(:epoch, 17, :loss, 9.325224f0)\n",
      " 14.046568 seconds (1.94 M allocations: 5.310 GiB, 42.93% gc time)\n",
      "(:epoch, 18, :loss, 8.728403f0)\n",
      " 14.796012 seconds (1.96 M allocations: 5.346 GiB, 41.53% gc time)\n",
      "(:epoch, 19, :loss, 8.255089f0)\n",
      " 13.510276 seconds (1.97 M allocations: 5.358 GiB, 39.39% gc time)\n",
      "(:epoch, 20, :loss, 7.742766f0)\n",
      " 10.736155 seconds (2.76 M allocations: 10.094 GiB, 9.73% gc time)\n",
      "(:epoch, 1, :loss, 24.37286f0)\n",
      "  8.506728 seconds (2.20 M allocations: 7.565 GiB, 8.69% gc time)\n",
      "(:epoch, 2, :loss, 27.15722f0)\n",
      "  7.853171 seconds (2.00 M allocations: 6.717 GiB, 8.55% gc time)\n",
      "(:epoch, 3, :loss, 28.333212f0)\n",
      "  8.152332 seconds (1.92 M allocations: 6.294 GiB, 9.05% gc time)\n",
      "(:epoch, 4, :loss, 27.583506f0)\n",
      "  9.039447 seconds (1.92 M allocations: 6.055 GiB, 8.56% gc time)\n",
      "(:epoch, 5, :loss, 27.069706f0)\n",
      "  9.649361 seconds (1.92 M allocations: 5.872 GiB, 9.61% gc time)\n",
      "(:epoch, 6, :loss, 26.683598f0)\n",
      "  8.877828 seconds (1.94 M allocations: 5.786 GiB, 9.98% gc time)\n",
      "(:epoch, 7, :loss, 26.10085f0)\n",
      "  9.819458 seconds (1.94 M allocations: 5.688 GiB, 20.59% gc time)\n",
      "(:epoch, 8, :loss, 24.227013f0)\n",
      " 10.910720 seconds (1.93 M allocations: 5.591 GiB, 22.59% gc time)\n",
      "(:epoch, 9, :loss, 22.062632f0)\n",
      " 13.358613 seconds (1.93 M allocations: 5.548 GiB, 30.52% gc time)\n",
      "(:epoch, 10, :loss, 20.266857f0)\n",
      " 13.033784 seconds (1.95 M allocations: 5.541 GiB, 35.57% gc time)\n",
      "(:epoch, 11, :loss, 18.942995f0)\n",
      " 11.017679 seconds (1.93 M allocations: 5.450 GiB, 39.81% gc time)\n",
      "(:epoch, 12, :loss, 17.15656f0)\n",
      " 11.197865 seconds (1.94 M allocations: 5.444 GiB, 40.76% gc time)\n",
      "(:epoch, 13, :loss, 15.83264f0)\n",
      " 12.901047 seconds (1.96 M allocations: 5.468 GiB, 44.82% gc time)\n",
      "(:epoch, 14, :loss, 14.900687f0)\n",
      " 16.318727 seconds (1.96 M allocations: 5.443 GiB, 44.84% gc time)\n",
      "(:epoch, 15, :loss, 13.61179f0)\n",
      " 17.379314 seconds (1.95 M allocations: 5.370 GiB, 42.17% gc time)\n",
      "(:epoch, 16, :loss, 12.510303f0)\n",
      " 16.376909 seconds (1.95 M allocations: 5.358 GiB, 41.71% gc time)\n"
     ]
    }
   ],
   "source": [
    "optim = map(x->Adam(lr=lr, gclip=gclip), model)\n",
    "# MAIN LOOP\n",
    "function trainingloop()\n",
    "    for epoch=1:epochs\n",
    "        @time train(model, data[1], optim, min(epoch,seqlength); pdrop=dpout)\n",
    "        # Calculate and print the losses after each epoch\n",
    "        losses = map(d->avgloss(model,d,100),data)\n",
    "        println((:epoch,epoch,:loss,losses...))\n",
    "    end\n",
    "end\n",
    "trainingloop()\n",
    "Profile.clear_malloc_data()\n",
    "trainingloop()\n",
    "# Profile.clear()\n",
    "# @profile trainingloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you have checked the loss decreasing, let's create some text with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## FINAL  MODEL OUTPUT ############\n",
      "the fthe pe depedl ldddddddddddedddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd\n"
     ]
    }
   ],
   "source": [
    "println(\"########## FINAL  MODEL OUTPUT ############\")\n",
    "state = initstate(model,1)\n",
    "generate(model, vocab, togenerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "open(\"gru_cpu_profile.bin\", \"w\") do f serialize(f, Profile.retrieve()) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
