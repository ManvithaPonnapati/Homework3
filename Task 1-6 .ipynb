{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 0.001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ProfileView\n",
    "using Knet, AutoGrad\n",
    "using Knet: sigm_dot, tanh_dot\n",
    "Profile.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles    = [\"input.txt\"]  # If provided, use first file for training, second for dev, others for test.\n",
    "togenerate   = 500            # If non-zero generate given number of characters.\n",
    "epochs       = 10             # Number of epochs for training.\n",
    "hidden       = [128]          # Sizes of one or more LSTM layers.\n",
    "embed        = 168            # Size of the embedding vector.\n",
    "batchsize    = 128            # Number of sequences to train on in parallel\n",
    "seqlength    = 20             # Maximum number of steps to unroll the network for bptt. Initial epochs will use the epoch number as bptt length for faster convergence.\n",
    "seed         = -1             # Random number seed. -1 or 0 is no fixed seed\n",
    "lr           = 1e-1           # Initial learning rate\n",
    "gclip        = 3.0            # Value to clip the gradient norm at.\n",
    "dpout        = 0.0            # Dropout probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mChars read: Tuple{String,Int64}[(\"input.txt\", 105989)]\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "seed > 0 && srand(seed)\n",
    "\n",
    "# read text and report lengths\n",
    "text = map(readstring, datafiles)\n",
    "!isempty(text) && info(\"Chars read: $(map((f,c)->(basename(f),length(c)),datafiles,text))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-1: Create dictionary by completing createVocabulary function\n",
    "function createVocabulary takes text::Array{Any,1} that contains the names of datafiles you provided by opts[:datafiles] argument. It returns vocabulary::Dict{Char,Int}() for given text. In this lab, your text array is length of 1. For example the text is [\"content of input\"]. Note that for the sake of simplicity, we do NOT use validation or test dataset in this lab. You can try it by splitting your data into 3 different set after the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createVocabulary (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function createVocabulary(text)\n",
    "    vocab = Dict{Char,Int}()\n",
    "    # MY CODE STARTS HERE \n",
    "    \n",
    "    for (char_i,unique_character) in enumerate(unique(text[1]))\n",
    "        vocab[Char(unique_character)] = char_i\n",
    "    end\n",
    "    # MY CODE ENDS HERE\n",
    "    return vocab\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36m75 unique chars.\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "vocab = createVocabulary(text)\n",
    "info(\"$(length(vocab)) unique chars.\") # The output should be 75 unique chars for input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network function\n",
    "\n",
    "In a regular RNN - The core idea is to use past hidden weights, present input to calculate the next set of hidden state weights. i.e st = (Ux/t + W/st-1) . \n",
    "\n",
    "LSTM equations looks scary and there are a lot of them forget gate, ingate , output gate and change . But it's basically just another way to calculate the hidden states (except in this scenario the vanishing gradients is not an issue). \n",
    "\n",
    "In a LSTM network you have initialize four sets of weights Wf (Forget gate ft) Wi - input gage, Wo - Output, Wc - Change gate weights and the corresponding biases. So instead of creating new variables looks like this cell code is just creating one giant weights/gates and then split it into parts of equal width(columnwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    hsize   = size(hidden,2)\n",
    "    forget  = sigm_dot(gates[:,1:hsize])\n",
    "    ingate  = sigm_dot(gates[:,1+hsize:2hsize])\n",
    "    outgate = sigm_dot(gates[:,1+2hsize:3hsize])\n",
    "    change  = tanh_dot(gates[:,1+3hsize:end])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh_dot(cell)\n",
    "    return (hidden,cell)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-2: Create Initial weights\n",
    "initweights creates the weights and biases for the model. We are using LSTM network. We provide init function(for weights) and bias function(for bias)\n",
    "\n",
    "First we have to initialize weights from the placeholder here x=embed and embed being 168 looks like the input vector is 168 \n",
    "long vector. The first part of this model is the relationshp betwen concatted inputs and hidden weights .This belongs to the embedding layer. Also for multiple hidden layers, y1=cell(x) , y2= cell(y1).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(hidden, vocab, embed)\n",
    "    init(d...) = xavier(d...)\n",
    "    bias(d...) = zeros(d...)\n",
    "    model = Vector{Any}(2*length(hidden)+3)\n",
    "    X = embed\n",
    "    for k = 1:length(hidden)\n",
    "        # MY CODE STARTS HERE\n",
    "        #Concatted input and hidden layer weights\n",
    "        num_nodes_hidden = hidden[k]\n",
    "        model[2k-1]=init(X+num_nodes_hidden,4*num_nodes_hidden) #Because we have to initialize 4w's - wf,wi,wo,wc\n",
    "        model[2k]=bias(1,4*num_nodes_hidden)\n",
    "        X = num_nodes_hidden\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "    model[end-2] = init(vocab,embed)\n",
    "    model[end-1] = init(hidden[end],vocab)\n",
    "    model[end] = bias(1,vocab)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-3: Create Initial state\n",
    "At each time step, we take the hidden state from previous time step as input. To be able to do that,first we need to initialize hidden state. We also store updated hidden states in array created here. We initialize state as a zero matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initstate (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let blank = nothing; global initstate\n",
    "    function initstate(model, batch)\n",
    "        nlayers = div(length(model)-3,2)\n",
    "        state = Vector{Any}(2*nlayers)\n",
    "        for k = 1:nlayers\n",
    "            bias = model[2k]\n",
    "            hidden = div(length(bias),4)\n",
    "            if typeof(blank)!=typeof(bias) || size(blank)!=(batch,hidden)\n",
    "                blank = fill!(similar(bias, batch, hidden),0)\n",
    "            end\n",
    "            state[2k-1] = state[2k] = blank\n",
    "        end\n",
    "        return state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-4: Create Predict function\n",
    "predict is a function that takes w(model) created in initweights, s(state) created in initstate and input whose size is batchsize vocabulary You need to implement predict function for LSTM. You must use lstm function here. LSTM function is provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(model, state, input; pdrop=0)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    newstate = similar(state)\n",
    "    for k = 1:nlayers\n",
    "        # MY CODE STARTS HERE\n",
    "        #newstate[2k-1] is the hidden layer (look at the initweights for more explanation)\n",
    "        input = dropout(input, pdrop)\n",
    "        (newstate[2k-1],newstate[2k])=lstm(model[2k-1],model[2k],state[2k-1],state[2k],input)\n",
    "        input = newstate[2k-1]\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "    return input,newstate\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Sample function\n",
    "Generate function is a function we use to create some text that is similar to our training data. We provide sample function to you. You can predict the next character by using sample function once you calculate the probabilities given the input. index to char is the same dictionary as you created with createdictionary function but it works in the reverse direction. It gives you the character given the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate(model, tok2int, nchar)\n",
    "    int2tok = Vector{Char}(length(tok2int))\n",
    "    for (k,v) in tok2int; int2tok[v] = k; end\n",
    "    input = tok2int[' ']\n",
    "    state = initstate(model, 1)\n",
    "    for t in 1:nchar\n",
    "        embed = model[end-2][[input],:]\n",
    "        ypred,state = predict(model,state,embed)\n",
    "        ypred = ypred * model[end-1] .+ model[end]\n",
    "        input = sample(exp.(logp(ypred)))\n",
    "        print(int2tok[input])\n",
    "    end\n",
    "    println()\n",
    "end\n",
    "\n",
    "function sample(p)\n",
    "    p = convert(Array,p)\n",
    "    r = rand()\n",
    "    for c = 1:length(p)\n",
    "        r -= p[c]\n",
    "        r < 0 && return c\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, Let's generate some random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## RANDOM MODEL OUTPUT ############\n",
      "J14':k,8OGhY:SA!pbC*.u*nA9MlgzWBxkh 4W\n",
      "&tD8a1iGdx6N9mB;}jbp:kUsBEj .PpO7wIgS*;vARmHq.Wu\n",
      "fgxm0.FgLfcDfvzywTEuj{;RK7'vsBG{PmTbc096kK{g\\&fk,FFlinq,y,31F9OSkJzjq\\C33a,7rNOcQ8JCPgUaDfjS;5FcOuJek5lf9u;4!w-,\n",
      "noazhatb8{W3lm:4S,yAHr9S:qy ,w4KNhu9 ?E9Hz:y2Hnxjn08n?H:9THWtYA1,HS2M\n",
      "pQzQ35KG*xRo79{RnTq.!M'mCviG8JoM;'!iaA5kRVpvmf02n-8w6?7c-zf6!sOvm;\n",
      "JKlb:\\VEh2bu-Dwp?C.n\n",
      "1-{LpIK6PiqVH20hVetHSJYK1;\n",
      "Bp1{&UJ9GM8EP'R83wAURAGRF0L{F!PSDBqY.;F3o{?&pCNMQ-g,zoVDOj7a13RvuwML\\Ox8xbBOkNUg4lahk1*wA:KY-7v!nD8foflFGp:lWoCazo\n"
     ]
    }
   ],
   "source": [
    "model = initweights(hidden, length(vocab), embed)\n",
    "state = initstate(model,1)\n",
    "\n",
    "println(\"########## RANDOM MODEL OUTPUT ############\")\n",
    "generate(model, vocab, togenerate) ## change togenerate if you want longer sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide minibatch function for you. You do not have to do it for this lab. But we suggest you to understand the idea since you need to do it in your own project and future labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function minibatch(chars, tok2int, batch_size)\n",
    "    chars = collect(chars)\n",
    "    nbatch = div(length(chars), batch_size)\n",
    "    data = [zeros(Int,batch_size) for i=1:nbatch ]\n",
    "    for n = 1:nbatch\n",
    "        for b = 1:batch_size\n",
    "            char = chars[(b-1)*nbatch + n]\n",
    "            data[n][b] = tok2int[char]\n",
    "        end\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-5: Create loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgloss (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, state, sequence, range=1:length(sequence)-1; newstate=nothing, pdrop=0)\n",
    "    preds = []\n",
    "    for t in range\n",
    "        input = model[end-2][sequence[t],:]\n",
    "        pred,state = predict(model,state,input; pdrop=pdrop)\n",
    "        push!(preds,pred)\n",
    "    end\n",
    "    if newstate != nothing\n",
    "        copy!(newstate, map(AutoGrad.getval,state))\n",
    "    end\n",
    "    pred0 = vcat(preds...)\n",
    "    pred1 = dropout(pred0,pdrop)\n",
    "    pred2 = pred1 * model[end-1]\n",
    "    pred3 = pred2 .+ model[end]\n",
    "    logp1 = logp(pred3,2)\n",
    "    nrows,ncols = size(pred3)\n",
    "    golds = vcat(sequence[range[1]+1:range[end]+1]...)\n",
    "    index = similar(golds)\n",
    "    @inbounds for i=1:length(golds)\n",
    "        index[i] = i + (golds[i]-1)*nrows\n",
    "    end\n",
    "    logp2 = logp1[index]\n",
    "    logp3 = sum(logp2)\n",
    "    return -logp3 / length(golds)\n",
    "end\n",
    "\n",
    "# Knet magic\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function avgloss(model, sequence, S)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    total = count = 0\n",
    "    for i in 1:S:T-1\n",
    "        j = min(i+S-1,T-1)\n",
    "        n = j-i+1\n",
    "        total += n * loss(model, state, sequence, i:j; newstate=state)\n",
    "        count += n\n",
    "    end\n",
    "    return total / count\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-6: Create Train function¶\n",
    "Implement bptt(Backpropagation through time) function for training. You need to fill up only 3 lines(or even small numbers). You need use lossgradient function and update! function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(model, sequence, optim, S; pdrop=0)\n",
    "    T = length(sequence)\n",
    "    B = length(sequence[1])\n",
    "    state = initstate(model, B)\n",
    "    for i in 1:S:T-1\n",
    "        # MY CODE STARTS HERE\n",
    "        end_seq = 1+S-1\n",
    "        if end_seq > T-1\n",
    "            end_seq = T-1\n",
    "        end\n",
    "        gradient_loss = lossgradient(model,state,sequence,1:end_seq,newstate=state,pdrop=pdrop)\n",
    "        update!(model,gradient_loss,optim)\n",
    "        # MY CODE ENDS HERE\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are ready. First let's see the initial loss¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 0, :loss, 4.317710789128025)\n"
     ]
    }
   ],
   "source": [
    "data =  map(t->minibatch(t, vocab, batchsize), text)\n",
    "# Print the loss of randomly initialized model.\n",
    "losses = map(d->avgloss(model,d,100), data)\n",
    "println((:epoch,0,:loss,losses...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the training part of RNN(with Adam)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17.978274 seconds (3.76 M allocations: 13.204 GiB, 9.36% gc time)\n",
      "(:epoch, 1, :loss, 6.500574832581594)\n",
      " 23.752375 seconds (1.82 M allocations: 10.573 GiB, 5.02% gc time)\n",
      "(:epoch, 2, :loss, 17.382165653140667)\n",
      " 25.468135 seconds (1.63 M allocations: 9.716 GiB, 5.83% gc time)\n",
      "(:epoch, 3, :loss, 15.570942705143533)\n",
      " 31.403616 seconds (1.56 M allocations: 9.288 GiB, 6.68% gc time)\n",
      "(:epoch, 4, :loss, 15.215044454339695)\n",
      " 33.061409 seconds (1.56 M allocations: 9.053 GiB, 18.52% gc time)\n",
      "(:epoch, 5, :loss, 13.405256737949621)\n",
      " 27.054769 seconds (1.56 M allocations: 8.861 GiB, 16.60% gc time)\n",
      "(:epoch, 6, :loss, 12.733036698299456)\n",
      " 30.068846 seconds (1.58 M allocations: 8.792 GiB, 21.99% gc time)\n",
      "(:epoch, 7, :loss, 11.294816060984461)\n",
      " 31.621162 seconds (1.58 M allocations: 8.689 GiB, 26.01% gc time)\n",
      "(:epoch, 8, :loss, 9.840545306015208)\n",
      " 30.375135 seconds (1.58 M allocations: 8.576 GiB, 29.15% gc time)\n",
      "(:epoch, 9, :loss, 8.93140280181636)\n",
      " 29.717287 seconds (1.58 M allocations: 8.540 GiB, 26.52% gc time)\n",
      "(:epoch, 10, :loss, 8.083292621621517)\n",
      " 25.424377 seconds (2.29 M allocations: 13.124 GiB, 6.31% gc time)\n",
      "(:epoch, 1, :loss, 15.53986754602283)\n",
      " 23.795862 seconds (1.78 M allocations: 10.571 GiB, 5.70% gc time)\n",
      "(:epoch, 2, :loss, 19.29539173858944)\n",
      " 21.440499 seconds (1.61 M allocations: 9.715 GiB, 6.83% gc time)\n",
      "(:epoch, 3, :loss, 21.606073276043748)\n",
      " 21.182431 seconds (1.53 M allocations: 9.287 GiB, 8.54% gc time)\n",
      "(:epoch, 4, :loss, 20.217206346258994)\n",
      " 19.058062 seconds (1.55 M allocations: 9.052 GiB, 8.25% gc time)\n"
     ]
    }
   ],
   "source": [
    "optim = map(x->Adam(lr=lr, gclip=gclip), model)\n",
    "# MAIN LOOP\n",
    "function trainingloop()\n",
    "    for epoch=1:epochs\n",
    "        @time train(model, data[1], optim, min(epoch,seqlength); pdrop=dpout)\n",
    "        # Calculate and print the losses after each epoch\n",
    "        losses = map(d->avgloss(model,d,100),data)\n",
    "        println((:epoch,epoch,:loss,losses...))\n",
    "    end\n",
    "end\n",
    "trainingloop()\n",
    "Profile.clear()\n",
    "@profile trainingloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you have checked the loss decreasing, let's create some text with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## FINAL  MODEL OUTPUT ############\n",
      "as en'ers, ntyeke.\\\n",
      "Mo Toooooooooooooooooooooooooooooooother,\\\n",
      "Wherwhoman,\\\n",
      "Then it oo veasplm: be be beteeeea centyeer.\\\n",
      "Man\\\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "MENENIA:\\\n",
      "M micingenoition b th ll ofur lld,\\\n",
      "Whe teerw\\\n",
      "Mat\\\n",
      "MENENIA:\\\n",
      "Mo head wal.\\\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "MENENIA: \\\n",
      "EW'd lanhad,\\\n",
      "Whicher voono'd,\\\n",
      "As alleveere\\\n",
      "\\\n",
      "\\\n",
      "e icssild,\\\n",
      "As alam co;oooooooooooooooonvevoicen bod\\\n",
      "Eves wetives whoughad,\\\n",
      "Whichou fol ol and g ashink\\\n",
      "UMNIA:\\\n",
      "Maspa; contyerw\\\n",
      "\\\n",
      "Soithe ppiIOLve'te,'twas whes ileto'd are;\\\n",
      "And lef fike. mand thinkinger; d,\\\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "println(\"########## FINAL  MODEL OUTPUT ############\")\n",
    "state = initstate(model,1)\n",
    "generate(model, vocab, togenerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"cpu_profile.bin\", \"w\") do f serialize(f, Profile.retrieve()) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
